{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import nengolib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import islice\n",
    "from IPython.display import clear_output\n",
    "import pytry\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and Save Input Data - Only run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dt = 1.0/30 #30 frames per second?\n",
    "\n",
    "#load combined file of annotated child-child data from PInSoRo\n",
    "data = pd.read_csv(\"multidata.csv\", low_memory=False) \n",
    "#array of data for purple child (points in space for each frame)\n",
    "x = np.array(data.iloc[:,11:195]).astype(float) \n",
    "#array of labels (purple child annotations, engagement) \n",
    "labs = np.array(data.iloc[:,443]).astype(str) \n",
    "a = []\n",
    "#for each column in the array, fill in any blanks by interpolating between available values\n",
    "for i in range(x.shape[1]):\n",
    "    y = pd.Series(x[:,i])\n",
    "    if i == 180:               \n",
    "        y[y>0]-=np.pi*2   \n",
    "        y += np.pi            \n",
    "    z = y.interpolate(limit_direction='both')\n",
    "    a.append(z)\n",
    "a = pd.DataFrame(a)\n",
    "a = a.dropna()\n",
    "a = np.array(a).T\n",
    "\n",
    "#function to extract consecutive frames with the same label (i.e. clips)\n",
    "def extract_pattern(start, end, target_dt): \n",
    "    pattern = np.array(a[start:end,:]).astype(float)\n",
    "    frames = np.array(data.iloc[start:end,9]).astype(int)\n",
    "    timestamp = np.array(data.iloc[start:end,0]).astype(str)\n",
    "\n",
    "    good_indices = frames != -1\n",
    "    frames = frames[good_indices]\n",
    "    pattern = pattern[good_indices]\n",
    "\n",
    "    fps = 30.0\n",
    "    t_sample = (frames - frames[0])/fps\n",
    "\n",
    "    #t = np.arange(int(t_sample[-1]/target_dt))*target_dt\n",
    "    t = np.arange(end-start)*target_dt\n",
    "\n",
    "    result = []\n",
    "    for i in range(pattern.shape[1]):       \n",
    "        p = np.interp(t, t_sample, pattern[:,i])\n",
    "        result.append(p)\n",
    "    result = np.array(result).T\n",
    "\n",
    "    return timestamp, result\n",
    "\n",
    "start=[]\n",
    "start.append(0)\n",
    "end=[]\n",
    "label = []\n",
    "for i in range(1, (len(labs)-1)):\n",
    "    if labs[i]!=labs[i-1]:\n",
    "        start.append(i)\n",
    "    if labs[i]!=labs[i+1]:\n",
    "        end.append(i)\n",
    "        label.append(labs[i])\n",
    "\n",
    "t_high=[]\n",
    "p_high=[]\n",
    "t_mid=[]\n",
    "p_mid=[]\n",
    "t_low=[]\n",
    "p_low=[]\n",
    "\n",
    "for i in range(1,(len(start)-1)):\n",
    "    try:\n",
    "        if label[i]==('goaloriented'):\n",
    "            ts, pi = extract_pattern(start[i], end[i], target_dt=target_dt)\n",
    "            t_high.append(ts)\n",
    "            p_high.append(pi)\n",
    "        if label[i]==('aimless'):\n",
    "            ts, pi = extract_pattern(start[i], end[i], target_dt=target_dt)\n",
    "            t_mid.append(ts)\n",
    "            p_mid.append(pi)\n",
    "        if label[i]==('noplay'):\n",
    "            ts, pi = extract_pattern(start[i], end[i], target_dt=target_dt)\n",
    "            t_low.append(ts)\n",
    "            p_low.append(pi)\n",
    "    except IndexError:\n",
    "        print('empty pattern')\n",
    "        \n",
    "i=0\n",
    "high = []\n",
    "mid = []\n",
    "low = []\n",
    "for entry in t_high:\n",
    "    high.append([t_high[i],p_high[i]])\n",
    "    i = i+1\n",
    "i=0\n",
    "for entry in t_mid:\n",
    "    mid.append([t_mid[i],p_mid[i]])\n",
    "    i = i+1\n",
    "i=0\n",
    "for entry in t_low:\n",
    "    low.append([t_low[i],p_low[i]])\n",
    "    i = i+1\n",
    "    \n",
    "np.save('higheng.npy', high)\n",
    "np.save('mideng.npy', mid)\n",
    "np.save('loweng.npy', low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternInterpolationTrain(pytry.Trial):\n",
    "    def params(self):\n",
    "        self.param('number of dimensions', n_dims=2),\n",
    "        self.param('length of training pattern', len_train=10),\n",
    "        self.param('number of epochs', n_epoch=1),\n",
    "        self.param('seed', p_seed=1),\n",
    "        \n",
    "    def evaluate(self, param): #function to fill in missing data points by interpolating\n",
    "        import nengo\n",
    "        high = list(np.load('higheng.npy'))\n",
    "        mid = list(np.load('mideng.npy'))\n",
    "        low = list(np.load('loweng.npy'))\n",
    "        \n",
    "        seed=param.p_seed\n",
    "\n",
    "        dt = 0.001\n",
    "        target_dt = 1.0/30 #data sampled at 30Hz\n",
    "        D = param.n_dims\n",
    "        classify_score = {}\n",
    "        accuracy = {}\n",
    "        \n",
    "        #shuffle data\n",
    "        p_high2 = random.sample(high, len(high))\n",
    "        p_low2 = random.sample(low, len(low))\n",
    "        p_mid2 = random.sample(mid, len(mid))\n",
    "        p_mid2 = np.array(p_mid2)\n",
    "        \n",
    "        #Create 80% training sets\n",
    "        high_train_ts = np.array(p_high2[:(int(len(p_high2)*0.8))]) \n",
    "        high_train_list=list(high_train_ts[:,1])\n",
    "        high_train_time=list(high_train_ts[:,0])\n",
    "        \n",
    "        low_train_ts = np.array(p_low2[:(int(len(p_low2)*0.8))]) \n",
    "        low_train_list=list(low_train_ts[:,1])\n",
    "        low_train_time=list(low_train_ts[:,0])\n",
    "        \n",
    "        #Create 20% testing sets\n",
    "        high_test_ts = np.array(p_high2[(int(len(p_high2)*0.8)):]) \n",
    "        high_test_list=list(high_test_ts[:,1])\n",
    "        high_test_time=list(high_test_ts[:,0])\n",
    "\n",
    "        low_test_ts = np.array(p_low2[(int(len(p_low2)*0.8)):]) \n",
    "        low_test_list=list(low_test_ts[:,1])\n",
    "        low_test_time=list(low_test_ts[:,0])\n",
    "        \n",
    "        #Remove empty patterns (only found in low engagement set)\n",
    "        for p in low_train_list:\n",
    "            if p.size > 0:\n",
    "                pass\n",
    "            else:\n",
    "                low_train_list.remove(p)\n",
    "\n",
    "        for p in low_test_list:\n",
    "            if p.size >0:\n",
    "                pass\n",
    "            else:\n",
    "                low_test_list.remove(p)\n",
    "\n",
    "        #Create PCA model using training data and save model\n",
    "        train_all = np.vstack(high_train_list+low_train_list)\n",
    "        pca_model = PCA(n_components=D).fit(train_all)\n",
    "        \n",
    "        pickle_filename = \"%s_pca_model.pkl\" % seed\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(pca_model, file)\n",
    "\n",
    "        #Save timestamps for unstacking data later\n",
    "        train_time_all = np.hstack(high_train_time+low_train_time)\n",
    "        test_time_all = np.hstack(high_test_time+low_test_time)\n",
    "        \n",
    "        np.save('%s_high_train_time.npy' % seed, high_train_time)\n",
    "        np.save('%s_low_train_time.npy' % seed, low_train_time)\n",
    "\n",
    "########## TRANSFORM TRAINING PATTERNS INTO PCA COMPONENTS #########\n",
    "        high_train_pca = np.vstack([pca_model.transform(p) for p in high_train_list])\n",
    "        low_train_pca = np.vstack([pca_model.transform(p) for p in low_train_list])\n",
    "\n",
    "        np.save('%s_hightrain_pca.npy' % seed, high_train_pca)\n",
    "        np.save('%s_low_train_pca.npy' % seed, low_train_pca)\n",
    "        \n",
    "########## TRANSFORM TESTING PATTERNS - ONE CLIP AT A TIME #########\n",
    "        high_test_pca2=[]\n",
    "        low_test_pca2=[]\n",
    "        mid_test_pca2=[]\n",
    "\n",
    "        for i in range(len(high_test_list)):\n",
    "            high_test_pca2.append(pca_model.transform(high_test_list[i]))\n",
    "\n",
    "        for i in range(len(low_test_list)):\n",
    "            low_test_pca2.append(pca_model.transform(low_test_list[i]))\n",
    "\n",
    "            \n",
    "        mid_list=list(p_mid2[:,1])\n",
    "        mid_time=list(p_mid2[:,0])\n",
    "        for i in range(len(mid_list)):\n",
    "            mid_test_pca2.append(pca_model.transform(mid_list[i]))\n",
    "            \n",
    "        #Save testing data\n",
    "        np.save('%s_high_test_pca2.npy' % seed, high_test_pca2)\n",
    "        np.save('%s_low_test_pca2.npy' % seed, low_test_pca2)\n",
    "        np.save('%s_mid_test_pca2.npy' % seed, mid_test_pca2)\n",
    "        \n",
    "        #Save timestamps\n",
    "        np.save('%s_high_test_time.npy' % seed, high_test_time)\n",
    "        np.save('%s_low_test_time.npy' % seed, low_test_time)\n",
    "        np.save('%s_mid_time.npy' % seed, mid_time)\n",
    "#############################################################\n",
    "\n",
    "        T_train = param.len_train   # number of seconds to train on for each class\n",
    "        T_test = 100    # number of seconds to test on for each class\n",
    "        \n",
    "        high_train_time2 = [item for sublist in high_train_time for item in sublist]\n",
    "        low_train_time2 = [item for sublist in low_train_time for item in sublist]\n",
    "\n",
    "        N_frames = int(T_train*30)\n",
    "        train_timestamps = np.hstack([high_train_time2[:(N_frames*param.n_epoch)], \n",
    "                                      low_train_time2[:(N_frames*param.n_epoch)]])\n",
    "        \n",
    "############ TRAINING WITH 80% ############ \n",
    "        print('Run Number: ', int(param.p_seed)+1)\n",
    "        print('Testing with 80% Training Data')\n",
    "    \n",
    "        num_epochs = param.n_epoch\n",
    "        batch_size = int(T_train*30)\n",
    "        batch = 0\n",
    "        theta = 15\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            training = np.vstack([high_train_pca[batch:int(batch+batch_size)], \n",
    "                                  low_train_pca[batch:int(batch+batch_size)]])\n",
    "            batch += batch_size\n",
    "\n",
    "            training_net = nengo.Network(seed=seed)#param.seed)\n",
    "            with training_net:\n",
    "                rw = []\n",
    "                for i in range(D):\n",
    "                    process = nengo.processes.PresentInput(np.hstack([high_train_pca[:,i], low_train_pca[:,i]]), \n",
    "                                                               presentation_time=1.0/30)\n",
    "                    rw.append(nengolib.networks.RollingWindow(theta=theta, n_neurons=3000, \n",
    "                                                              process=process, \n",
    "                                                              neuron_type=nengo.Direct()))\n",
    "\n",
    "\n",
    "                node_pool = nengo.Node(None, size_in=rw[0].state.size_out*D)\n",
    "\n",
    "                start = 0\n",
    "                for r in rw:\n",
    "                    nengo.Connection(r.state, node_pool[start:start+r.state.size_out])\n",
    "                    start += r.state.size_out\n",
    "\n",
    "\n",
    "\n",
    "                stim = nengo.Node(nengo.processes.PresentInput(training, presentation_time=1.0/30))\n",
    "                assert stim.size_out == D\n",
    "                for i in range(D):\n",
    "                    nengo.Connection(stim[i], rw[i].input, synapse=None)\n",
    "\n",
    "                p_node_pool = nengo.Probe(node_pool, sample_every = 0.1)\n",
    "\n",
    "\n",
    "            sim = nengo.Simulator(training_net)\n",
    "            with sim:\n",
    "                sim.run(T_train*2)  \n",
    "\n",
    "\n",
    "            pool_model = nengo.Network()\n",
    "            with pool_model:\n",
    "                pool = nengo.Ensemble(n_neurons=3000, dimensions=node_pool.size_out,\n",
    "                                      neuron_type=nengo.LIFRate(), seed=seed)\n",
    "            pool_sim = nengo.Simulator(pool_model)\n",
    "\n",
    "            import nengo.utils.ensemble\n",
    "\n",
    "            _, a = nengo.utils.ensemble.tuning_curves(pool, pool_sim, inputs=sim.data[p_node_pool])\n",
    "\n",
    "            if epoch == 0:\n",
    "                a_high = a[:(int(len(a)/2))]\n",
    "                a_low = a[(int(len(a)/2)):]\n",
    "            else:\n",
    "                a_high = np.concatenate((a_high, a[:(int(len(a)/2))]))\n",
    "                a_low = np.concatenate((a_low, a[(int(len(a)/2)):]))\n",
    "\n",
    "        a_out = np.vstack([a_high, a_low])\n",
    "\n",
    "\n",
    "        N = int((len(a_out))/2) #int(T_train*1000)\n",
    "        target = np.hstack([np.ones(N), -np.ones(N)]).reshape(-1, 1)\n",
    "        dec, info = nengo.solvers.LstsqL2(reg=0.1)(a_out, target)\n",
    "        \n",
    "        np.save('%s_decoder.npy' % seed, dec)\n",
    "        np.save('%s_node_pool.npy' % seed, node_pool.size_out)\n",
    "        \n",
    "        v = np.dot(a_out, dec)\n",
    "\n",
    "\n",
    "        train_timestamps = train_timestamps[0::2]\n",
    "\n",
    "        output_high_train = {\"output_high_train\":{\"time\":train_timestamps[:N], \"output\":v[:N]}}\n",
    "            \n",
    "        output_low_train = {\"output_low_train\":{\"time\":train_timestamps[N:], \"output\":v[N:]}}\n",
    "\n",
    "############ SAVE DATA ############ \n",
    "       \n",
    "        results = {**output_high_train, **output_low_train}\n",
    "\n",
    "        return results            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for seed in range(20):\n",
    "    PatternInterpolationTrain().run(seed=seed, \n",
    "                                    p_seed=seed, \n",
    "                                    len_train=500, \n",
    "                                    n_epoch=3, \n",
    "                                    data_dir='Training_Out', \n",
    "                                    data_format=\"npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
