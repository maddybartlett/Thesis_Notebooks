{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbartlett2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nengo_dl\\converter.py:305: UserWarning: Layer '<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'>' already has a converter. Overwriting.\n",
      "  \"Layer '%s' already has a converter. Overwriting.\" % keras_layer\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime, os\n",
    "from itertools import islice\n",
    "from IPython.display import clear_output\n",
    "import pytry\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import nengo_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Nengo Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow version 2.2.0 <br>\n",
    "Nengo dl version 3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTrain(pytry.Trial):\n",
    "    def params(self):\n",
    "        self.param('run_n', run_n=1)\n",
    "        self.param('theta', theta=3)\n",
    "        self.param('q', q=4)\n",
    "        self.param('lr', lr=0.001)\n",
    "        self.param('epoch', epoch=1000)\n",
    "    \n",
    "    def evaluate(self, param):       \n",
    "        #LOAD/SET PARAMETERS\n",
    "        run_n=param.run_n     \n",
    "        theta=param.theta  \n",
    "        q=param.q  \n",
    "        learn_rate=param.lr #learning rate\n",
    "        n_epoch=param.epoch #number of training epochs\n",
    "        train_rat=0.7 #ratio of training to testing data\n",
    "        \n",
    "        print(run_n) #print run number\n",
    "        np.random.seed() #set seed to random value\n",
    "        \n",
    "#################################################################\n",
    "        #Set and create data directories\n",
    "        data_dir=\"D:\\\\NEN002\\\\GridSearch_NDL_LMU\\\\\"+str(learn_rate) #directory to save data to\n",
    "        \n",
    "        if run_n == 0:\n",
    "            os.mkdir(data_dir)\n",
    "            os.mkdir(\"D:\\\\NEN002\\\\GridSearch_NDL_LMU_logs\\\\\"+str(learn_rate))\n",
    "            \n",
    "        logdir = os.path.join(\"D:\\\\NEN002\\\\GridSearch_NDL_LMU_logs\\\\\"+str(learn_rate)+\"\\\\\"+str(run_n)) #directory to save logs to for tensorboard\n",
    "        \n",
    "        #load data\n",
    "        highengdata = np.load('high_lmu'+str(q)+'_theta'+str(theta)+'.npy',allow_pickle=True) #np.load('higheng.npy', allow_pickle=True)\n",
    "        lowengdata = np.load('low_lmu'+str(q)+'_theta'+str(theta)+'.npy',allow_pickle=True) #np.load('loweng.npy', allow_pickle=True)\n",
    "##################################################################\n",
    "\n",
    "        #high=list(highengdata[:])\n",
    "        #low=list(lowengdata[:])\n",
    "        \n",
    "        high=list(highengdata) #[:,1])\n",
    "        low=list(lowengdata) #[:,1])\n",
    "\n",
    "        #shuffle clips\n",
    "        np.random.shuffle(high)\n",
    "        np.random.shuffle(low)\n",
    "\n",
    "        #get training clips and convert to list of frames\n",
    "        high_train = high[:(int(len(low)*train_rat))]\n",
    "        high_train=list(np.vstack(high_train))\n",
    "        low_train = low[:(int(len(low)*train_rat))]\n",
    "        low_train=list(np.vstack(low_train))\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(high_train)\n",
    "        np.random.shuffle(low_train)\n",
    "\n",
    "        #Make each training set same size (low is shortest)\n",
    "        high_train = high_train[:(int(len(low_train)))]\n",
    "        low_train = low_train[:(int(len(low_train)))]\n",
    "\n",
    "        #Create 10% validation dataset\n",
    "        high_val = high[(int(len(low)*0.9)):(int(len(low)*1))]\n",
    "        low_val = low[(int(len(low)*0.9)):(int(len(low)*1))]\n",
    "        \n",
    "        #convert to list of frames\n",
    "        high_val=list(np.vstack(high_val))\n",
    "        low_val=list(np.vstack(low_val))\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(high_val)\n",
    "        np.random.shuffle(low_val)\n",
    "        \n",
    "        #create test sets\n",
    "        #get 30% of clips for testing\n",
    "        high_test = high[(int(len(low)*train_rat)):(int(len(low)*0.9))]\n",
    "        low_test = low[(int(len(low)*train_rat)):(int(len(low)*0.9))]\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(high_test)\n",
    "        np.random.shuffle(low_test)\n",
    "\n",
    "        #GENERATE INPUT MATRICES\n",
    "        #Concatenate high and low sets together to create a single array for training, test and validation separately\n",
    "        all_train = np.vstack(np.concatenate((high_train, low_train)))\n",
    "        all_test = np.concatenate((high_test, low_test)) \n",
    "        all_val = np.concatenate((high_val, low_val))\n",
    "\n",
    "        #create the target data for training\n",
    "        target_train = np.zeros((all_train.shape[0],2))\n",
    "        n_high = len(high_train)\n",
    "        target_train[:n_high,0] = 1 #target for high = [1,0]\n",
    "        target_train[n_high:,1] = 1 #target for low = [0,1]\n",
    "        \n",
    "        #and for validation\n",
    "        target_val = np.zeros((all_val.shape[0],2))\n",
    "        n_high = len(high_val)\n",
    "        target_val[:n_high,0] = 1\n",
    "        target_val[n_high:,1] = 1\n",
    "        \n",
    "        #SAVE TRAINING AND TESTING DATA\n",
    "        pickle_filename = (data_dir+\"/%s_training_data.pkl\") % str(run_n) #(data_dir+\"/%s_training_data\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(all_train, file)\n",
    "\n",
    "        pickle_filename = (data_dir+\"/%s_testing_data.pkl\") % str(run_n) #(data_dir+\"/%s_testing_data\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(all_test, file)\n",
    "\n",
    "        #BUILD MODEL (input layer -> hidden layer -> output layer)\n",
    "        N = 200\n",
    "        seed = 1\n",
    "        N_dims = len(all_train[0]) \n",
    "\n",
    "        model = nengo.Network(seed=seed)\n",
    "        with model:    \n",
    "            input = nengo.Node(np.zeros(N_dims))\n",
    "            hidden1 = nengo.Ensemble(n_neurons=N, dimensions=N_dims, radius=np.sqrt(N_dims), \n",
    "                                     neuron_type=nengo.RectifiedLinear())\n",
    "            nengo.Connection(input, hidden1, synapse=None)\n",
    "            output = nengo.Node(None, size_in=2)\n",
    "            nengo.Connection(hidden1, output, eval_points=all_train, function=target_train, \n",
    "                             scale_eval_points=False, synapse=None)\n",
    "\n",
    "            p_output = nengo.Probe(output)\n",
    "        \n",
    "        #TRAINING DATA AND PARAMETERS\n",
    "        minibatch_size = 1000       # this can be adjusted to speed up training\n",
    "                                    # Note: with larger minibatch_size, you may need to reduce this\n",
    "                                    #(if training is making things worse, then you need to reduce this!)\n",
    "                                    # number of iterations through the training data to perform\n",
    "\n",
    "        batches = int(np.ceil(len(all_train)/minibatch_size)) #number of training batches needed (length of data / minibatch size)\n",
    "        val_batches = int(np.ceil(len(all_val)/minibatch_size)) #number of validation batches needed\n",
    "\n",
    "        training_data_input = np.array(all_train, copy=True)\n",
    "        order = np.arange(len(training_data_input))\n",
    "        np.random.shuffle(order)\n",
    "        training_data_input_rand = training_data_input[order]\n",
    "        training_data_input_rand.resize(minibatch_size,batches,N_dims)\n",
    "        training_data_input.resize(minibatch_size,batches,N_dims)\n",
    "\n",
    "        training_data_output = np.array(target_train, copy=True)\n",
    "        training_data_output_rand = training_data_output[order]\n",
    "        training_data_output.resize(minibatch_size,batches,2)\n",
    "        training_data_output_rand.resize(minibatch_size,batches,2)\n",
    "        \n",
    "        val_input = np.array(all_val, copy=True)\n",
    "        order = np.arange(len(val_input))\n",
    "        np.random.shuffle(order)\n",
    "        val_input_rand = val_input[order]\n",
    "        val_input_rand.resize(minibatch_size,val_batches,N_dims)\n",
    "        val_input.resize(minibatch_size,val_batches,N_dims)\n",
    "        \n",
    "        val_output = np.array(target_val, copy=True)\n",
    "        val_output_rand = val_output[order]\n",
    "        val_output.resize(minibatch_size,val_batches,2)\n",
    "        val_output_rand.resize(minibatch_size,val_batches,2)\n",
    "        \n",
    "        #set learning objective (defines loss)\n",
    "        def objective(outputs, targets): \n",
    "            return tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=outputs, labels=targets)  \n",
    "        \n",
    "        #TRAINING\n",
    "        with nengo_dl.Simulator(\n",
    "                model, minibatch_size=minibatch_size) as sim:\n",
    "            sim.compile(optimizer=tf.optimizers.Adam(learn_rate),\n",
    "                loss={p_output: objective})\n",
    "            sim.fit(training_data_input_rand, training_data_output_rand, epochs=n_epoch,\n",
    "                   callbacks=[tf.keras.callbacks.TensorBoard(log_dir=logdir)],\n",
    "                   validation_data=(val_input_rand, val_output_rand))\n",
    "            sim.save_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n #('./trained.data') \n",
    "            \n",
    "        #Run model with training data, using trained weights\n",
    "        input.output = nengo.processes.PresentInput(all_train, presentation_time=0.001)\n",
    "        with nengo_dl.Simulator(model, minibatch_size=1) as sim:\n",
    "            sim.load_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n\n",
    "            sim.run(len(all_train)*0.001)\n",
    "            \n",
    "        #Save data\n",
    "        pickle_filename = (data_dir+\"/%s_train_target.pkl\") % str(run_n) #(data_dir+\"/%s_train_target\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(target_train, file)\n",
    "        \n",
    "        pickle_filename = (data_dir+\"/%s_train_trange.pkl\") % str(run_n) #(data_dir+\"/%s_train_trange\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.trange(), file)\n",
    "            \n",
    "        pickle_filename = (data_dir+\"/%s_train_out.pkl\") % str(run_n) #(data_dir+\"/%s_train_out\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.data[p_output][0], file)    \n",
    "        \n",
    "        test_out=[]\n",
    "        target_test=[]\n",
    "        for j in range(len(all_test)):\n",
    "            if j<int(len(high_test)):\n",
    "                target = np.zeros((all_test[j].shape[0],2))\n",
    "                target[:,0] = 1\n",
    "            else:\n",
    "                target = np.zeros((all_test[j].shape[0],2))\n",
    "                target[:,1] = 1\n",
    "            input.output = nengo.processes.PresentInput(all_test[j], presentation_time=0.001)\n",
    "            with nengo_dl.Simulator(model, minibatch_size=1) as sim:\n",
    "                sim.load_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n\n",
    "                sim.run(len(all_test[j])*0.001)\n",
    "            test_out.append(sim.data[p_output][0])\n",
    "            target_test.append(target)\n",
    "            \n",
    "        #Save data\n",
    "        pickle_filename = (data_dir+\"/%s_test_target.pkl\") % str(run_n) #(data_dir+\"/%s_test_target\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(target_test, file)\n",
    "        \n",
    "        pickle_filename = (data_dir+\"/%s_test_trange.pkl\") % str(run_n) #(data_dir+\"/%s_test_trange\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.trange(), file)\n",
    "            \n",
    "        pickle_filename = (data_dir+\"/%s_test_out.pkl\") % str(run_n) #(data_dir+\"/%s_test_out\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(test_out, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running FeedForwardTrain#20200720-105345-0b1a8c39\n",
      "9\n",
      "|                     Building network (0%)                    | ETA:  --:--:--"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbartlett2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nengo_dl\\simulator.py:460: UserWarning: No GPU support detected. It is recommended that you install tensorflow-gpu (`pip install tensorflow-gpu`).\n",
      "  \"No GPU support detected. It is recommended that you \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:02                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer TensorGraph has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer TensorGraph has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.8014 - probe_loss: 0.8014 - val_loss: -1587.0345 - val_probe_loss: -1587.0345\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 1s 937us/sample - loss: -1583.2781 - probe_loss: -1583.2781 - val_loss: -3176.9194 - val_probe_loss: -3176.9194\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 1s 833us/sample - loss: -3205.2412 - probe_loss: -3205.2412 - val_loss: -4774.3823 - val_probe_loss: -4774.3823\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 1s 826us/sample - loss: -4881.9946 - probe_loss: -4881.9946 - val_loss: -6382.0430 - val_probe_loss: -6382.0430\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 1s 890us/sample - loss: -6629.8496 - probe_loss: -6629.8496 - val_loss: -8002.2847 - val_probe_loss: -8002.2847\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 1s 891us/sample - loss: -8468.8193 - probe_loss: -8468.8193 - val_loss: -9635.2725 - val_probe_loss: -9635.2725\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -10423.8936 - probe_loss: -10423.8936 - val_loss: -11284.0918 - val_probe_loss: -11284.0918\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 1s 875us/sample - loss: -12524.2109 - probe_loss: -12524.2109 - val_loss: -12958.0049 - val_probe_loss: -12958.0049\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 1s 805us/sample - loss: -14803.7002 - probe_loss: -14803.7002 - val_loss: -14675.7881 - val_probe_loss: -14675.7881\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 1s 769us/sample - loss: -17304.5605 - probe_loss: -17304.5605 - val_loss: -16467.1055 - val_probe_loss: -16467.1055\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 1s 738us/sample - loss: -20076.5547 - probe_loss: -20076.5547 - val_loss: -18380.3379 - val_probe_loss: -18380.3379\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 1s 737us/sample - loss: -23176.0859 - probe_loss: -23176.0859 - val_loss: -20472.4395 - val_probe_loss: -20472.4395\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 1s 768us/sample - loss: -26667.5625 - probe_loss: -26667.5625 - val_loss: -22805.7285 - val_probe_loss: -22805.7285\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 1s 927us/sample - loss: -30621.6875 - probe_loss: -30621.6875 - val_loss: -25435.3848 - val_probe_loss: -25435.3848\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 1s 778us/sample - loss: -35113.9922 - probe_loss: -35113.9922 - val_loss: -28414.1367 - val_probe_loss: -28414.1367\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -40225.7539 - probe_loss: -40225.7539 - val_loss: -31789.0645 - val_probe_loss: -31789.0645\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 1s 902us/sample - loss: -46045.8789 - probe_loss: -46045.8789 - val_loss: -35612.9453 - val_probe_loss: -35612.9453\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 1s 905us/sample - loss: -52674.2070 - probe_loss: -52674.2070 - val_loss: -39937.3516 - val_probe_loss: -39937.3516\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 1s 886us/sample - loss: -60214.9688 - probe_loss: -60214.9688 - val_loss: -44817.2344 - val_probe_loss: -44817.2344\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 1s 899us/sample - loss: -68773.6406 - probe_loss: -68773.6406 - val_loss: -50307.0898 - val_probe_loss: -50307.0898\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 1s 831us/sample - loss: -78463.3984 - probe_loss: -78463.3984 - val_loss: -56472.6016 - val_probe_loss: -56472.6016\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 1s 927us/sample - loss: -89400.8984 - probe_loss: -89400.8984 - val_loss: -63369.1445 - val_probe_loss: -63369.1445\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 1s 915us/sample - loss: -101709.0000 - probe_loss: -101709.0000 - val_loss: -71055.1797 - val_probe_loss: -71055.1797\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 1s 942us/sample - loss: -115517.0781 - probe_loss: -115517.0781 - val_loss: -79608.3359 - val_probe_loss: -79608.3359\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 1s 919us/sample - loss: -130964.4922 - probe_loss: -130964.4922 - val_loss: -89100.2188 - val_probe_loss: -89100.2188\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 1s 853us/sample - loss: -148198.5781 - probe_loss: -148198.5781 - val_loss: -99601.4453 - val_probe_loss: -99601.4453\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 1s 917us/sample - loss: -167373.4062 - probe_loss: -167373.4062 - val_loss: -111197.7266 - val_probe_loss: -111197.7266\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 1s 979us/sample - loss: -188650.7188 - probe_loss: -188650.7188 - val_loss: -123975.1719 - val_probe_loss: -123975.1719\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 1s 968us/sample - loss: -212199.8750 - probe_loss: -212199.8750 - val_loss: -138025.2656 - val_probe_loss: -138025.2656\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 1s 777us/sample - loss: -238198.2344 - probe_loss: -238198.2344 - val_loss: -153442.1562 - val_probe_loss: -153442.1562\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -266831.6875 - probe_loss: -266831.6875 - val_loss: -170325.4531 - val_probe_loss: -170325.4531\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 1s 845us/sample - loss: -298295.2500 - probe_loss: -298295.2500 - val_loss: -188778.3906 - val_probe_loss: -188778.3906\n",
      "Epoch 33/1000\n",
      "1000/1000 [==============================] - 1s 872us/sample - loss: -332791.0625 - probe_loss: -332791.0625 - val_loss: -208908.0625 - val_probe_loss: -208908.0625\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -370531.2500 - probe_loss: -370531.2500 - val_loss: -230831.7188 - val_probe_loss: -230831.7188\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 1s 831us/sample - loss: -411737.3125 - probe_loss: -411737.3125 - val_loss: -254668.6094 - val_probe_loss: -254668.6094\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 1s 815us/sample - loss: -456641.6250 - probe_loss: -456641.6250 - val_loss: -280539.5312 - val_probe_loss: -280539.5312\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 1s 889us/sample - loss: -505483.3125 - probe_loss: -505483.3125 - val_loss: -308569.7500 - val_probe_loss: -308569.7500\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 1s 896us/sample - loss: -558512.2500 - probe_loss: -558512.2500 - val_loss: -338892.2188 - val_probe_loss: -338892.2188\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 1s 817us/sample - loss: -615985.6875 - probe_loss: -615985.6875 - val_loss: -371643.0938 - val_probe_loss: -371643.0938\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 1s 897us/sample - loss: -678168.0000 - probe_loss: -678168.0000 - val_loss: -406965.1875 - val_probe_loss: -406965.1875\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 1s 754us/sample - loss: -745335.0625 - probe_loss: -745335.0625 - val_loss: -445006.8750 - val_probe_loss: -445006.8750\n",
      "Epoch 42/1000\n",
      "1000/1000 [==============================] - 1s 770us/sample - loss: -817770.3750 - probe_loss: -817770.3750 - val_loss: -485920.5000 - val_probe_loss: -485920.5000\n",
      "Epoch 43/1000\n",
      "1000/1000 [==============================] - 1s 889us/sample - loss: -895768.6875 - probe_loss: -895768.6875 - val_loss: -529864.1250 - val_probe_loss: -529864.1250\n",
      "Epoch 44/1000\n",
      "1000/1000 [==============================] - 1s 743us/sample - loss: -979633.8125 - probe_loss: -979633.8125 - val_loss: -576998.6250 - val_probe_loss: -576998.6250\n",
      "Epoch 45/1000\n",
      "1000/1000 [==============================] - 1s 898us/sample - loss: -1069681.6250 - probe_loss: -1069681.6250 - val_loss: -627489.5000 - val_probe_loss: -627489.5000\n",
      "Epoch 46/1000\n",
      "1000/1000 [==============================] - 1s 945us/sample - loss: -1166235.8750 - probe_loss: -1166235.8750 - val_loss: -681512.1250 - val_probe_loss: -681512.1250\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 946us/sample - loss: -1269634.7500 - probe_loss: -1269634.7500 - val_loss: -739246.6875 - val_probe_loss: -739246.6875\n",
      "Epoch 48/1000\n",
      "1000/1000 [==============================] - 1s 951us/sample - loss: -1380227.0000 - probe_loss: -1380227.0000 - val_loss: -800872.6875 - val_probe_loss: -800872.6875\n",
      "Epoch 49/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1498366.6250 - probe_loss: -1498366.6250 - val_loss: -866580.7500 - val_probe_loss: -866580.7500\n",
      "Epoch 50/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1624418.6250 - probe_loss: -1624418.6250 - val_loss: -936572.6250 - val_probe_loss: -936572.6250\n",
      "Epoch 51/1000\n",
      "1000/1000 [==============================] - 1s 927us/sample - loss: -1758762.7500 - probe_loss: -1758762.7500 - val_loss: -1011055.0000 - val_probe_loss: -1011055.0000\n",
      "Epoch 52/1000\n",
      "1000/1000 [==============================] - 1s 978us/sample - loss: -1901787.0000 - probe_loss: -1901787.0000 - val_loss: -1090237.1250 - val_probe_loss: -1090237.1250\n",
      "Epoch 53/1000\n",
      "1000/1000 [==============================] - 1s 954us/sample - loss: -2053892.8750 - probe_loss: -2053892.8750 - val_loss: -1174338.2500 - val_probe_loss: -1174338.2500\n",
      "Epoch 54/1000\n",
      "1000/1000 [==============================] - 1s 957us/sample - loss: -2215496.2500 - probe_loss: -2215496.2500 - val_loss: -1263586.2500 - val_probe_loss: -1263586.2500\n",
      "Epoch 55/1000\n",
      "1000/1000 [==============================] - 1s 991us/sample - loss: -2387025.2500 - probe_loss: -2387025.2500 - val_loss: -1358205.7500 - val_probe_loss: -1358205.7500\n",
      "Epoch 56/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -2568913.7500 - probe_loss: -2568913.7500 - val_loss: -1458427.6250 - val_probe_loss: -1458427.6250\n",
      "Epoch 57/1000\n",
      "1000/1000 [==============================] - 1s 753us/sample - loss: -2761608.7500 - probe_loss: -2761608.7500 - val_loss: -1564489.7500 - val_probe_loss: -1564489.7500\n",
      "Epoch 58/1000\n",
      "1000/1000 [==============================] - 1s 771us/sample - loss: -2965562.7500 - probe_loss: -2965562.7500 - val_loss: -1676650.8750 - val_probe_loss: -1676650.8750\n",
      "Epoch 59/1000\n",
      "1000/1000 [==============================] - 1s 770us/sample - loss: -3181246.0000 - probe_loss: -3181246.0000 - val_loss: -1795160.6250 - val_probe_loss: -1795160.6250\n",
      "Epoch 60/1000\n",
      "1000/1000 [==============================] - 1s 828us/sample - loss: -3409129.2500 - probe_loss: -3409129.2500 - val_loss: -1920271.7500 - val_probe_loss: -1920271.7500\n",
      "Epoch 61/1000\n",
      "1000/1000 [==============================] - 1s 776us/sample - loss: -3649694.5000 - probe_loss: -3649694.5000 - val_loss: -2052251.1250 - val_probe_loss: -2052251.1250\n",
      "Epoch 62/1000\n",
      "1000/1000 [==============================] - 1s 834us/sample - loss: -3903430.7500 - probe_loss: -3903430.7500 - val_loss: -2191363.7500 - val_probe_loss: -2191363.7500\n",
      "Epoch 63/1000\n",
      "1000/1000 [==============================] - 1s 813us/sample - loss: -4170827.5000 - probe_loss: -4170827.5000 - val_loss: -2337880.2500 - val_probe_loss: -2337880.2500\n",
      "Epoch 64/1000\n",
      "1000/1000 [==============================] - 1s 924us/sample - loss: -4452387.5000 - probe_loss: -4452387.5000 - val_loss: -2492067.7500 - val_probe_loss: -2492067.7500\n",
      "Epoch 65/1000\n",
      "1000/1000 [==============================] - 1s 930us/sample - loss: -4748612.0000 - probe_loss: -4748612.0000 - val_loss: -2654197.5000 - val_probe_loss: -2654197.5000\n",
      "Epoch 66/1000\n",
      "1000/1000 [==============================] - 1s 890us/sample - loss: -5060019.0000 - probe_loss: -5060019.0000 - val_loss: -2824544.0000 - val_probe_loss: -2824544.0000\n",
      "Epoch 67/1000\n",
      "1000/1000 [==============================] - 1s 896us/sample - loss: -5387136.0000 - probe_loss: -5387136.0000 - val_loss: -3003402.5000 - val_probe_loss: -3003402.5000\n",
      "Epoch 68/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -5730498.0000 - probe_loss: -5730498.0000 - val_loss: -3191065.2500 - val_probe_loss: -3191065.2500\n",
      "Epoch 69/1000\n",
      "1000/1000 [==============================] - 1s 975us/sample - loss: -6090655.5000 - probe_loss: -6090655.5000 - val_loss: -3387834.7500 - val_probe_loss: -3387834.7500\n",
      "Epoch 70/1000\n",
      "1000/1000 [==============================] - 1s 975us/sample - loss: -6468168.5000 - probe_loss: -6468168.5000 - val_loss: -3594024.7500 - val_probe_loss: -3594024.7500\n",
      "Epoch 71/1000\n",
      "1000/1000 [==============================] - 1s 753us/sample - loss: -6863616.5000 - probe_loss: -6863616.5000 - val_loss: -3809936.0000 - val_probe_loss: -3809936.0000\n",
      "Epoch 72/1000\n",
      "1000/1000 [==============================] - 1s 829us/sample - loss: -7277570.5000 - probe_loss: -7277570.5000 - val_loss: -4035883.0000 - val_probe_loss: -4035883.0000\n",
      "Epoch 73/1000\n",
      "1000/1000 [==============================] - 1s 789us/sample - loss: -7710618.0000 - probe_loss: -7710618.0000 - val_loss: -4272197.5000 - val_probe_loss: -4272197.5000\n",
      "Epoch 74/1000\n",
      "1000/1000 [==============================] - 1s 791us/sample - loss: -8163355.0000 - probe_loss: -8163355.0000 - val_loss: -4519200.0000 - val_probe_loss: -4519200.0000\n",
      "Epoch 75/1000\n",
      "1000/1000 [==============================] - 1s 767us/sample - loss: -8636379.0000 - probe_loss: -8636379.0000 - val_loss: -4777230.5000 - val_probe_loss: -4777230.5000\n",
      "Epoch 76/1000\n",
      "1000/1000 [==============================] - 1s 862us/sample - loss: -9130314.0000 - probe_loss: -9130314.0000 - val_loss: -5046638.5000 - val_probe_loss: -5046638.5000\n",
      "Epoch 77/1000\n",
      "1000/1000 [==============================] - 1s 847us/sample - loss: -9645784.0000 - probe_loss: -9645784.0000 - val_loss: -5327756.0000 - val_probe_loss: -5327756.0000\n",
      "Epoch 78/1000\n",
      "1000/1000 [==============================] - 1s 832us/sample - loss: -10183424.0000 - probe_loss: -10183424.0000 - val_loss: -5620948.0000 - val_probe_loss: -5620948.0000\n",
      "Epoch 79/1000\n",
      "1000/1000 [==============================] - 1s 859us/sample - loss: -10743875.0000 - probe_loss: -10743875.0000 - val_loss: -5926571.5000 - val_probe_loss: -5926571.5000\n",
      "Epoch 80/1000\n",
      "1000/1000 [==============================] - 1s 924us/sample - loss: -11327778.0000 - probe_loss: -11327778.0000 - val_loss: -6244977.0000 - val_probe_loss: -6244977.0000\n",
      "Epoch 81/1000\n",
      "1000/1000 [==============================] - 1s 963us/sample - loss: -11935782.0000 - probe_loss: -11935782.0000 - val_loss: -6576550.0000 - val_probe_loss: -6576550.0000\n",
      "Epoch 82/1000\n",
      "1000/1000 [==============================] - 1s 953us/sample - loss: -12568571.0000 - probe_loss: -12568571.0000 - val_loss: -6921681.0000 - val_probe_loss: -6921681.0000\n",
      "Epoch 83/1000\n",
      "1000/1000 [==============================] - 1s 895us/sample - loss: -13226827.0000 - probe_loss: -13226827.0000 - val_loss: -7280771.0000 - val_probe_loss: -7280771.0000\n",
      "Epoch 84/1000\n",
      "1000/1000 [==============================] - 1s 873us/sample - loss: -13911252.0000 - probe_loss: -13911252.0000 - val_loss: -7654242.0000 - val_probe_loss: -7654242.0000\n",
      "Epoch 85/1000\n",
      "1000/1000 [==============================] - 1s 740us/sample - loss: -14622563.0000 - probe_loss: -14622563.0000 - val_loss: -8042520.5000 - val_probe_loss: -8042520.5000\n",
      "Epoch 86/1000\n",
      "1000/1000 [==============================] - 1s 915us/sample - loss: -15361484.0000 - probe_loss: -15361484.0000 - val_loss: -8446046.0000 - val_probe_loss: -8446046.0000\n",
      "Epoch 87/1000\n",
      "1000/1000 [==============================] - 1s 827us/sample - loss: -16128755.0000 - probe_loss: -16128755.0000 - val_loss: -8865250.0000 - val_probe_loss: -8865250.0000\n",
      "Epoch 88/1000\n",
      "1000/1000 [==============================] - 1s 799us/sample - loss: -16925112.0000 - probe_loss: -16925112.0000 - val_loss: -9300559.0000 - val_probe_loss: -9300559.0000\n",
      "Epoch 89/1000\n",
      "1000/1000 [==============================] - 1s 801us/sample - loss: -17751316.0000 - probe_loss: -17751316.0000 - val_loss: -9752423.0000 - val_probe_loss: -9752423.0000\n",
      "Epoch 90/1000\n",
      "1000/1000 [==============================] - 1s 970us/sample - loss: -18608128.0000 - probe_loss: -18608128.0000 - val_loss: -10221281.0000 - val_probe_loss: -10221281.0000\n",
      "Epoch 91/1000\n",
      "1000/1000 [==============================] - 1s 924us/sample - loss: -19496330.0000 - probe_loss: -19496330.0000 - val_loss: -10707604.0000 - val_probe_loss: -10707604.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/1000\n",
      "1000/1000 [==============================] - 1s 831us/sample - loss: -20416726.0000 - probe_loss: -20416726.0000 - val_loss: -11211882.0000 - val_probe_loss: -11211882.0000\n",
      "Epoch 93/1000\n",
      "1000/1000 [==============================] - 1s 836us/sample - loss: -21370112.0000 - probe_loss: -21370112.0000 - val_loss: -11734540.0000 - val_probe_loss: -11734540.0000\n",
      "Epoch 94/1000\n",
      "1000/1000 [==============================] - 1s 781us/sample - loss: -22357272.0000 - probe_loss: -22357272.0000 - val_loss: -12276047.0000 - val_probe_loss: -12276047.0000\n",
      "Epoch 95/1000\n",
      "1000/1000 [==============================] - 1s 827us/sample - loss: -23379010.0000 - probe_loss: -23379010.0000 - val_loss: -12836862.0000 - val_probe_loss: -12836862.0000\n",
      "Epoch 96/1000\n",
      "1000/1000 [==============================] - 1s 890us/sample - loss: -24436124.0000 - probe_loss: -24436124.0000 - val_loss: -13417470.0000 - val_probe_loss: -13417470.0000\n",
      "Epoch 97/1000\n",
      "1000/1000 [==============================] - 1s 924us/sample - loss: -25529410.0000 - probe_loss: -25529410.0000 - val_loss: -14018351.0000 - val_probe_loss: -14018351.0000\n",
      "Epoch 98/1000\n",
      "1000/1000 [==============================] - 1s 718us/sample - loss: -26659680.0000 - probe_loss: -26659680.0000 - val_loss: -14639969.0000 - val_probe_loss: -14639969.0000\n",
      "Epoch 99/1000\n",
      "1000/1000 [==============================] - 1s 892us/sample - loss: -27827782.0000 - probe_loss: -27827782.0000 - val_loss: -15282787.0000 - val_probe_loss: -15282787.0000\n",
      "Epoch 100/1000\n",
      "1000/1000 [==============================] - 1s 762us/sample - loss: -29034510.0000 - probe_loss: -29034510.0000 - val_loss: -15947302.0000 - val_probe_loss: -15947302.0000\n",
      "Epoch 101/1000\n",
      "1000/1000 [==============================] - 1s 759us/sample - loss: -30280698.0000 - probe_loss: -30280698.0000 - val_loss: -16634017.0000 - val_probe_loss: -16634017.0000\n",
      "Epoch 102/1000\n",
      "1000/1000 [==============================] - 1s 714us/sample - loss: -31567170.0000 - probe_loss: -31567170.0000 - val_loss: -17343394.0000 - val_probe_loss: -17343394.0000\n",
      "Epoch 103/1000\n",
      "1000/1000 [==============================] - 1s 840us/sample - loss: -32894772.0000 - probe_loss: -32894772.0000 - val_loss: -18075904.0000 - val_probe_loss: -18075904.0000\n",
      "Epoch 104/1000\n",
      "1000/1000 [==============================] - 1s 843us/sample - loss: -34264312.0000 - probe_loss: -34264312.0000 - val_loss: -18832042.0000 - val_probe_loss: -18832042.0000\n",
      "Epoch 105/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -35676640.0000 - probe_loss: -35676640.0000 - val_loss: -19612288.0000 - val_probe_loss: -19612288.0000\n",
      "Epoch 106/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -37132588.0000 - probe_loss: -37132588.0000 - val_loss: -20417094.0000 - val_probe_loss: -20417094.0000\n",
      "Epoch 107/1000\n",
      "1000/1000 [==============================] - 1s 815us/sample - loss: -38632988.0000 - probe_loss: -38632988.0000 - val_loss: -21246986.0000 - val_probe_loss: -21246986.0000\n",
      "Epoch 108/1000\n",
      "1000/1000 [==============================] - 1s 897us/sample - loss: -40178660.0000 - probe_loss: -40178660.0000 - val_loss: -22102406.0000 - val_probe_loss: -22102406.0000\n",
      "Epoch 109/1000\n",
      "1000/1000 [==============================] - 1s 886us/sample - loss: -41770424.0000 - probe_loss: -41770424.0000 - val_loss: -22983898.0000 - val_probe_loss: -22983898.0000\n",
      "Epoch 110/1000\n",
      "1000/1000 [==============================] - 1s 776us/sample - loss: -43409152.0000 - probe_loss: -43409152.0000 - val_loss: -23891940.0000 - val_probe_loss: -23891940.0000\n",
      "Epoch 111/1000\n",
      "1000/1000 [==============================] - 1s 766us/sample - loss: -45095708.0000 - probe_loss: -45095708.0000 - val_loss: -24827042.0000 - val_probe_loss: -24827042.0000\n",
      "Epoch 112/1000\n",
      "1000/1000 [==============================] - 1s 740us/sample - loss: -46830944.0000 - probe_loss: -46830944.0000 - val_loss: -25789778.0000 - val_probe_loss: -25789778.0000\n",
      "Epoch 113/1000\n",
      "1000/1000 [==============================] - 1s 866us/sample - loss: -48615780.0000 - probe_loss: -48615780.0000 - val_loss: -26780612.0000 - val_probe_loss: -26780612.0000\n",
      "Epoch 114/1000\n",
      "1000/1000 [==============================] - 1s 900us/sample - loss: -50451068.0000 - probe_loss: -50451068.0000 - val_loss: -27800038.0000 - val_probe_loss: -27800038.0000\n",
      "Epoch 115/1000\n",
      "1000/1000 [==============================] - 1s 856us/sample - loss: -52337676.0000 - probe_loss: -52337676.0000 - val_loss: -28848536.0000 - val_probe_loss: -28848536.0000\n",
      "Epoch 116/1000\n",
      "1000/1000 [==============================] - 1s 777us/sample - loss: -54276472.0000 - probe_loss: -54276472.0000 - val_loss: -29926672.0000 - val_probe_loss: -29926672.0000\n",
      "Epoch 117/1000\n",
      "1000/1000 [==============================] - 1s 838us/sample - loss: -56268360.0000 - probe_loss: -56268360.0000 - val_loss: -31034948.0000 - val_probe_loss: -31034948.0000\n",
      "Epoch 118/1000\n",
      "1000/1000 [==============================] - 1s 765us/sample - loss: -58314232.0000 - probe_loss: -58314232.0000 - val_loss: -32173878.0000 - val_probe_loss: -32173878.0000\n",
      "Epoch 119/1000\n",
      "1000/1000 [==============================] - 1s 832us/sample - loss: -60414972.0000 - probe_loss: -60414972.0000 - val_loss: -33343986.0000 - val_probe_loss: -33343986.0000\n",
      "Epoch 120/1000\n",
      "1000/1000 [==============================] - 1s 928us/sample - loss: -62571468.0000 - probe_loss: -62571468.0000 - val_loss: -34545856.0000 - val_probe_loss: -34545856.0000\n",
      "Epoch 121/1000\n",
      "1000/1000 [==============================] - 1s 908us/sample - loss: -64784640.0000 - probe_loss: -64784640.0000 - val_loss: -35779992.0000 - val_probe_loss: -35779992.0000\n",
      "Epoch 122/1000\n",
      "1000/1000 [==============================] - 1s 884us/sample - loss: -67055404.0000 - probe_loss: -67055404.0000 - val_loss: -37046908.0000 - val_probe_loss: -37046908.0000\n",
      "Epoch 123/1000\n",
      "1000/1000 [==============================] - 1s 769us/sample - loss: -69384640.0000 - probe_loss: -69384640.0000 - val_loss: -38347088.0000 - val_probe_loss: -38347088.0000\n",
      "Epoch 124/1000\n",
      "1000/1000 [==============================] - 1s 931us/sample - loss: -71773224.0000 - probe_loss: -71773224.0000 - val_loss: -39681144.0000 - val_probe_loss: -39681144.0000\n",
      "Epoch 125/1000\n",
      "1000/1000 [==============================] - 1s 827us/sample - loss: -74222120.0000 - probe_loss: -74222120.0000 - val_loss: -41049596.0000 - val_probe_loss: -41049596.0000\n",
      "Epoch 126/1000\n",
      "1000/1000 [==============================] - 1s 945us/sample - loss: -76732168.0000 - probe_loss: -76732168.0000 - val_loss: -42453024.0000 - val_probe_loss: -42453024.0000\n",
      "Epoch 127/1000\n",
      "1000/1000 [==============================] - 1s 738us/sample - loss: -79304304.0000 - probe_loss: -79304304.0000 - val_loss: -43891996.0000 - val_probe_loss: -43891996.0000\n",
      "Epoch 128/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -81939384.0000 - probe_loss: -81939384.0000 - val_loss: -45367084.0000 - val_probe_loss: -45367084.0000\n",
      "Epoch 129/1000\n",
      "1000/1000 [==============================] - 1s 988us/sample - loss: -84638312.0000 - probe_loss: -84638312.0000 - val_loss: -46878864.0000 - val_probe_loss: -46878864.0000\n",
      "Epoch 130/1000\n",
      "1000/1000 [==============================] - 1s 994us/sample - loss: -87401984.0000 - probe_loss: -87401984.0000 - val_loss: -48427896.0000 - val_probe_loss: -48427896.0000\n",
      "Epoch 131/1000\n",
      "1000/1000 [==============================] - 1s 892us/sample - loss: -90231296.0000 - probe_loss: -90231296.0000 - val_loss: -50014760.0000 - val_probe_loss: -50014760.0000\n",
      "Epoch 132/1000\n",
      "1000/1000 [==============================] - 1s 932us/sample - loss: -93127152.0000 - probe_loss: -93127152.0000 - val_loss: -51639972.0000 - val_probe_loss: -51639972.0000\n",
      "Epoch 133/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -96090408.0000 - probe_loss: -96090408.0000 - val_loss: -53304280.0000 - val_probe_loss: -53304280.0000\n",
      "Epoch 134/1000\n",
      "1000/1000 [==============================] - 1s 933us/sample - loss: -99122096.0000 - probe_loss: -99122096.0000 - val_loss: -55008192.0000 - val_probe_loss: -55008192.0000\n",
      "Epoch 135/1000\n",
      "1000/1000 [==============================] - 1s 948us/sample - loss: -102223072.0000 - probe_loss: -102223072.0000 - val_loss: -56752284.0000 - val_probe_loss: -56752284.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/1000\n",
      "1000/1000 [==============================] - 1s 911us/sample - loss: -105394248.0000 - probe_loss: -105394248.0000 - val_loss: -58537148.0000 - val_probe_loss: -58537148.0000\n",
      "Epoch 137/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -108636576.0000 - probe_loss: -108636576.0000 - val_loss: -60363300.0000 - val_probe_loss: -60363300.0000\n",
      "Epoch 138/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -111950984.0000 - probe_loss: -111950984.0000 - val_loss: -62231292.0000 - val_probe_loss: -62231292.0000\n",
      "Epoch 139/1000\n",
      "1000/1000 [==============================] - 1s 988us/sample - loss: -115338384.0000 - probe_loss: -115338384.0000 - val_loss: -64141684.0000 - val_probe_loss: -64141684.0000\n",
      "Epoch 140/1000\n",
      "1000/1000 [==============================] - 1s 862us/sample - loss: -118799744.0000 - probe_loss: -118799744.0000 - val_loss: -66095012.0000 - val_probe_loss: -66095012.0000\n",
      "Epoch 141/1000\n",
      "1000/1000 [==============================] - 1s 865us/sample - loss: -122335984.0000 - probe_loss: -122335984.0000 - val_loss: -68091872.0000 - val_probe_loss: -68091872.0000\n",
      "Epoch 142/1000\n",
      "1000/1000 [==============================] - 1s 813us/sample - loss: -125948064.0000 - probe_loss: -125948064.0000 - val_loss: -70132936.0000 - val_probe_loss: -70132936.0000\n",
      "Epoch 143/1000\n",
      "1000/1000 [==============================] - 1s 779us/sample - loss: -129636912.0000 - probe_loss: -129636912.0000 - val_loss: -72218808.0000 - val_probe_loss: -72218808.0000\n",
      "Epoch 144/1000\n",
      "1000/1000 [==============================] - 1s 936us/sample - loss: -133403456.0000 - probe_loss: -133403456.0000 - val_loss: -74350080.0000 - val_probe_loss: -74350080.0000\n",
      "Epoch 145/1000\n",
      "1000/1000 [==============================] - 1s 875us/sample - loss: -137248576.0000 - probe_loss: -137248576.0000 - val_loss: -76527376.0000 - val_probe_loss: -76527376.0000\n",
      "Epoch 146/1000\n",
      "1000/1000 [==============================] - 1s 896us/sample - loss: -141173168.0000 - probe_loss: -141173168.0000 - val_loss: -78751312.0000 - val_probe_loss: -78751312.0000\n",
      "Epoch 147/1000\n",
      "1000/1000 [==============================] - 1s 963us/sample - loss: -145178192.0000 - probe_loss: -145178192.0000 - val_loss: -81022472.0000 - val_probe_loss: -81022472.0000\n",
      "Epoch 148/1000\n",
      "1000/1000 [==============================] - 1s 900us/sample - loss: -149264512.0000 - probe_loss: -149264512.0000 - val_loss: -83341536.0000 - val_probe_loss: -83341536.0000\n",
      "Epoch 149/1000\n",
      "1000/1000 [==============================] - 1s 953us/sample - loss: -153433088.0000 - probe_loss: -153433088.0000 - val_loss: -85708928.0000 - val_probe_loss: -85708928.0000\n",
      "Epoch 150/1000\n",
      "1000/1000 [==============================] - 1s 836us/sample - loss: -157684736.0000 - probe_loss: -157684736.0000 - val_loss: -88125376.0000 - val_probe_loss: -88125376.0000\n",
      "Epoch 151/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -162020416.0000 - probe_loss: -162020416.0000 - val_loss: -90591240.0000 - val_probe_loss: -90591240.0000\n",
      "Epoch 152/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -166440928.0000 - probe_loss: -166440928.0000 - val_loss: -93107312.0000 - val_probe_loss: -93107312.0000\n",
      "Epoch 153/1000\n",
      "1000/1000 [==============================] - 1s 840us/sample - loss: -170947248.0000 - probe_loss: -170947248.0000 - val_loss: -95674032.0000 - val_probe_loss: -95674032.0000\n",
      "Epoch 154/1000\n",
      "1000/1000 [==============================] - 1s 732us/sample - loss: -175540144.0000 - probe_loss: -175540144.0000 - val_loss: -98292144.0000 - val_probe_loss: -98292144.0000\n",
      "Epoch 155/1000\n",
      "1000/1000 [==============================] - 1s 797us/sample - loss: -180220736.0000 - probe_loss: -180220736.0000 - val_loss: -100962144.0000 - val_probe_loss: -100962144.0000\n",
      "Epoch 156/1000\n",
      "1000/1000 [==============================] - 1s 897us/sample - loss: -184989760.0000 - probe_loss: -184989760.0000 - val_loss: -103684832.0000 - val_probe_loss: -103684832.0000\n",
      "Epoch 157/1000\n",
      "1000/1000 [==============================] - 1s 782us/sample - loss: -189848320.0000 - probe_loss: -189848320.0000 - val_loss: -106460576.0000 - val_probe_loss: -106460576.0000\n",
      "Epoch 158/1000\n",
      "1000/1000 [==============================] - 1s 857us/sample - loss: -194797152.0000 - probe_loss: -194797152.0000 - val_loss: -109290264.0000 - val_probe_loss: -109290264.0000\n",
      "Epoch 159/1000\n",
      "1000/1000 [==============================] - 1s 852us/sample - loss: -199837264.0000 - probe_loss: -199837264.0000 - val_loss: -112174304.0000 - val_probe_loss: -112174304.0000\n",
      "Epoch 160/1000\n",
      "1000/1000 [==============================] - 1s 815us/sample - loss: -204969408.0000 - probe_loss: -204969408.0000 - val_loss: -115113440.0000 - val_probe_loss: -115113440.0000\n",
      "Epoch 161/1000\n",
      "1000/1000 [==============================] - 1s 856us/sample - loss: -210194608.0000 - probe_loss: -210194608.0000 - val_loss: -118108056.0000 - val_probe_loss: -118108056.0000\n",
      "Epoch 162/1000\n",
      "1000/1000 [==============================] - 1s 909us/sample - loss: -215513536.0000 - probe_loss: -215513536.0000 - val_loss: -121158896.0000 - val_probe_loss: -121158896.0000\n",
      "Epoch 163/1000\n",
      "1000/1000 [==============================] - 1s 786us/sample - loss: -220927264.0000 - probe_loss: -220927264.0000 - val_loss: -124266304.0000 - val_probe_loss: -124266304.0000\n",
      "Epoch 164/1000\n",
      "1000/1000 [==============================] - 1s 697us/sample - loss: -226436480.0000 - probe_loss: -226436480.0000 - val_loss: -127431064.0000 - val_probe_loss: -127431064.0000\n",
      "Epoch 165/1000\n",
      "1000/1000 [==============================] - 1s 840us/sample - loss: -232042224.0000 - probe_loss: -232042224.0000 - val_loss: -130653504.0000 - val_probe_loss: -130653504.0000\n",
      "Epoch 166/1000\n",
      "1000/1000 [==============================] - 1s 830us/sample - loss: -237745168.0000 - probe_loss: -237745168.0000 - val_loss: -133934440.0000 - val_probe_loss: -133934440.0000\n",
      "Epoch 167/1000\n",
      "1000/1000 [==============================] - 1s 755us/sample - loss: -243546352.0000 - probe_loss: -243546352.0000 - val_loss: -137274144.0000 - val_probe_loss: -137274144.0000\n",
      "Epoch 168/1000\n",
      "1000/1000 [==============================] - 1s 838us/sample - loss: -249446416.0000 - probe_loss: -249446416.0000 - val_loss: -140673392.0000 - val_probe_loss: -140673392.0000\n",
      "Epoch 169/1000\n",
      "1000/1000 [==============================] - 1s 881us/sample - loss: -255446368.0000 - probe_loss: -255446368.0000 - val_loss: -144132448.0000 - val_probe_loss: -144132448.0000\n",
      "Epoch 170/1000\n",
      "1000/1000 [==============================] - 1s 843us/sample - loss: -261546976.0000 - probe_loss: -261546976.0000 - val_loss: -147652096.0000 - val_probe_loss: -147652096.0000\n",
      "Epoch 171/1000\n",
      "1000/1000 [==============================] - 1s 785us/sample - loss: -267749088.0000 - probe_loss: -267749088.0000 - val_loss: -151232720.0000 - val_probe_loss: -151232720.0000\n",
      "Epoch 172/1000\n",
      "1000/1000 [==============================] - 1s 870us/sample - loss: -274053568.0000 - probe_loss: -274053568.0000 - val_loss: -154874672.0000 - val_probe_loss: -154874672.0000\n",
      "Epoch 173/1000\n",
      "1000/1000 [==============================] - 1s 914us/sample - loss: -280460992.0000 - probe_loss: -280460992.0000 - val_loss: -158578736.0000 - val_probe_loss: -158578736.0000\n",
      "Epoch 174/1000\n",
      "1000/1000 [==============================] - 1s 766us/sample - loss: -286972480.0000 - probe_loss: -286972480.0000 - val_loss: -162345248.0000 - val_probe_loss: -162345248.0000\n",
      "Epoch 175/1000\n",
      "1000/1000 [==============================] - 1s 786us/sample - loss: -293588704.0000 - probe_loss: -293588704.0000 - val_loss: -166174784.0000 - val_probe_loss: -166174784.0000\n",
      "Epoch 176/1000\n",
      "1000/1000 [==============================] - 1s 871us/sample - loss: -300310464.0000 - probe_loss: -300310464.0000 - val_loss: -170067808.0000 - val_probe_loss: -170067808.0000\n",
      "Epoch 177/1000\n",
      "1000/1000 [==============================] - 1s 923us/sample - loss: -307138656.0000 - probe_loss: -307138656.0000 - val_loss: -174024800.0000 - val_probe_loss: -174024800.0000\n",
      "Epoch 178/1000\n",
      "1000/1000 [==============================] - 1s 781us/sample - loss: -314074048.0000 - probe_loss: -314074048.0000 - val_loss: -178046512.0000 - val_probe_loss: -178046512.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "1000/1000 [==============================] - 1s 793us/sample - loss: -321117536.0000 - probe_loss: -321117536.0000 - val_loss: -182132976.0000 - val_probe_loss: -182132976.0000\n",
      "Epoch 180/1000\n",
      "1000/1000 [==============================] - 1s 785us/sample - loss: -328269760.0000 - probe_loss: -328269760.0000 - val_loss: -186284912.0000 - val_probe_loss: -186284912.0000\n",
      "Epoch 181/1000\n",
      "1000/1000 [==============================] - 1s 680us/sample - loss: -335531520.0000 - probe_loss: -335531520.0000 - val_loss: -190502816.0000 - val_probe_loss: -190502816.0000\n",
      "Epoch 182/1000\n",
      "1000/1000 [==============================] - 1s 794us/sample - loss: -342903648.0000 - probe_loss: -342903648.0000 - val_loss: -194787360.0000 - val_probe_loss: -194787360.0000\n",
      "Epoch 183/1000\n",
      "1000/1000 [==============================] - 1s 742us/sample - loss: -350387168.0000 - probe_loss: -350387168.0000 - val_loss: -199138640.0000 - val_probe_loss: -199138640.0000\n",
      "Epoch 184/1000\n",
      "1000/1000 [==============================] - 1s 754us/sample - loss: -357982592.0000 - probe_loss: -357982592.0000 - val_loss: -203557328.0000 - val_probe_loss: -203557328.0000\n",
      "Epoch 185/1000\n",
      "1000/1000 [==============================] - 1s 820us/sample - loss: -365690816.0000 - probe_loss: -365690816.0000 - val_loss: -208043968.0000 - val_probe_loss: -208043968.0000\n",
      "Epoch 186/1000\n",
      "1000/1000 [==============================] - 1s 766us/sample - loss: -373512608.0000 - probe_loss: -373512608.0000 - val_loss: -212599232.0000 - val_probe_loss: -212599232.0000\n",
      "Epoch 187/1000\n",
      "1000/1000 [==============================] - 1s 726us/sample - loss: -381448896.0000 - probe_loss: -381448896.0000 - val_loss: -217223136.0000 - val_probe_loss: -217223136.0000\n",
      "Epoch 188/1000\n",
      "1000/1000 [==============================] - 1s 788us/sample - loss: -389500224.0000 - probe_loss: -389500224.0000 - val_loss: -221916432.0000 - val_probe_loss: -221916432.0000\n",
      "Epoch 189/1000\n",
      "1000/1000 [==============================] - 1s 859us/sample - loss: -397667520.0000 - probe_loss: -397667520.0000 - val_loss: -226679776.0000 - val_probe_loss: -226679776.0000\n",
      "Epoch 190/1000\n",
      "1000/1000 [==============================] - 1s 713us/sample - loss: -405951712.0000 - probe_loss: -405951712.0000 - val_loss: -231513296.0000 - val_probe_loss: -231513296.0000\n",
      "Epoch 191/1000\n",
      "1000/1000 [==============================] - 1s 796us/sample - loss: -414353280.0000 - probe_loss: -414353280.0000 - val_loss: -236417584.0000 - val_probe_loss: -236417584.0000\n",
      "Epoch 192/1000\n",
      "1000/1000 [==============================] - 1s 780us/sample - loss: -422873120.0000 - probe_loss: -422873120.0000 - val_loss: -241393168.0000 - val_probe_loss: -241393168.0000\n",
      "Epoch 193/1000\n",
      "1000/1000 [==============================] - 1s 843us/sample - loss: -431512032.0000 - probe_loss: -431512032.0000 - val_loss: -246440688.0000 - val_probe_loss: -246440688.0000\n",
      "Epoch 194/1000\n",
      "1000/1000 [==============================] - 1s 767us/sample - loss: -440270880.0000 - probe_loss: -440270880.0000 - val_loss: -251560304.0000 - val_probe_loss: -251560304.0000\n",
      "Epoch 195/1000\n",
      "1000/1000 [==============================] - 1s 829us/sample - loss: -449150304.0000 - probe_loss: -449150304.0000 - val_loss: -256752592.0000 - val_probe_loss: -256752592.0000\n",
      "Epoch 196/1000\n",
      "1000/1000 [==============================] - 1s 742us/sample - loss: -458151040.0000 - probe_loss: -458151040.0000 - val_loss: -262018304.0000 - val_probe_loss: -262018304.0000\n",
      "Epoch 197/1000\n",
      "1000/1000 [==============================] - 1s 833us/sample - loss: -467274144.0000 - probe_loss: -467274144.0000 - val_loss: -267357456.0000 - val_probe_loss: -267357456.0000\n",
      "Epoch 198/1000\n",
      "1000/1000 [==============================] - 1s 779us/sample - loss: -476520064.0000 - probe_loss: -476520064.0000 - val_loss: -272770784.0000 - val_probe_loss: -272770784.0000\n",
      "Epoch 199/1000\n",
      "1000/1000 [==============================] - 1s 765us/sample - loss: -485889664.0000 - probe_loss: -485889664.0000 - val_loss: -278258784.0000 - val_probe_loss: -278258784.0000\n",
      "Epoch 200/1000\n",
      "1000/1000 [==============================] - 1s 727us/sample - loss: -495383808.0000 - probe_loss: -495383808.0000 - val_loss: -283822048.0000 - val_probe_loss: -283822048.0000\n",
      "Epoch 201/1000\n",
      "1000/1000 [==============================] - 1s 766us/sample - loss: -505003360.0000 - probe_loss: -505003360.0000 - val_loss: -289460576.0000 - val_probe_loss: -289460576.0000\n",
      "Epoch 202/1000\n",
      "1000/1000 [==============================] - 1s 876us/sample - loss: -514748832.0000 - probe_loss: -514748832.0000 - val_loss: -295175072.0000 - val_probe_loss: -295175072.0000\n",
      "Epoch 203/1000\n",
      "1000/1000 [==============================] - 1s 972us/sample - loss: -524621056.0000 - probe_loss: -524621056.0000 - val_loss: -300966208.0000 - val_probe_loss: -300966208.0000\n",
      "Epoch 204/1000\n",
      "1000/1000 [==============================] - 1s 942us/sample - loss: -534621152.0000 - probe_loss: -534621152.0000 - val_loss: -306834048.0000 - val_probe_loss: -306834048.0000\n",
      "Epoch 205/1000\n",
      "1000/1000 [==============================] - 1s 768us/sample - loss: -544749248.0000 - probe_loss: -544749248.0000 - val_loss: -312779136.0000 - val_probe_loss: -312779136.0000\n",
      "Epoch 206/1000\n",
      "1000/1000 [==============================] - 1s 803us/sample - loss: -555006464.0000 - probe_loss: -555006464.0000 - val_loss: -318802080.0000 - val_probe_loss: -318802080.0000\n",
      "Epoch 207/1000\n",
      "1000/1000 [==============================] - 1s 824us/sample - loss: -565393536.0000 - probe_loss: -565393536.0000 - val_loss: -324903360.0000 - val_probe_loss: -324903360.0000\n",
      "Epoch 208/1000\n",
      "1000/1000 [==============================] - 1s 848us/sample - loss: -575911424.0000 - probe_loss: -575911424.0000 - val_loss: -331083328.0000 - val_probe_loss: -331083328.0000\n",
      "Epoch 209/1000\n",
      "1000/1000 [==============================] - 1s 868us/sample - loss: -586560512.0000 - probe_loss: -586560512.0000 - val_loss: -337342784.0000 - val_probe_loss: -337342784.0000\n",
      "Epoch 210/1000\n",
      "1000/1000 [==============================] - 1s 936us/sample - loss: -597341952.0000 - probe_loss: -597341952.0000 - val_loss: -343681600.0000 - val_probe_loss: -343681600.0000\n",
      "Epoch 211/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -608256192.0000 - probe_loss: -608256192.0000 - val_loss: -350100608.0000 - val_probe_loss: -350100608.0000\n",
      "Epoch 212/1000\n",
      "1000/1000 [==============================] - 1s 885us/sample - loss: -619304128.0000 - probe_loss: -619304128.0000 - val_loss: -356600160.0000 - val_probe_loss: -356600160.0000\n",
      "Epoch 213/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -630486400.0000 - probe_loss: -630486400.0000 - val_loss: -363180800.0000 - val_probe_loss: -363180800.0000\n",
      "Epoch 214/1000\n",
      "1000/1000 [==============================] - 1s 879us/sample - loss: -641803904.0000 - probe_loss: -641803904.0000 - val_loss: -369842720.0000 - val_probe_loss: -369842720.0000\n",
      "Epoch 215/1000\n",
      "1000/1000 [==============================] - 1s 941us/sample - loss: -653257216.0000 - probe_loss: -653257216.0000 - val_loss: -376586656.0000 - val_probe_loss: -376586656.0000\n",
      "Epoch 216/1000\n",
      "1000/1000 [==============================] - 1s 902us/sample - loss: -664847296.0000 - probe_loss: -664847296.0000 - val_loss: -383413376.0000 - val_probe_loss: -383413376.0000\n",
      "Epoch 217/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -676575104.0000 - probe_loss: -676575104.0000 - val_loss: -390322368.0000 - val_probe_loss: -390322368.0000\n",
      "Epoch 218/1000\n",
      "1000/1000 [==============================] - 1s 739us/sample - loss: -688440768.0000 - probe_loss: -688440768.0000 - val_loss: -397314592.0000 - val_probe_loss: -397314592.0000\n",
      "Epoch 219/1000\n",
      "1000/1000 [==============================] - 1s 733us/sample - loss: -700445376.0000 - probe_loss: -700445376.0000 - val_loss: -404390560.0000 - val_probe_loss: -404390560.0000\n",
      "Epoch 220/1000\n",
      "1000/1000 [==============================] - 1s 935us/sample - loss: -712589696.0000 - probe_loss: -712589696.0000 - val_loss: -411550496.0000 - val_probe_loss: -411550496.0000\n",
      "Epoch 221/1000\n",
      "1000/1000 [==============================] - 1s 892us/sample - loss: -724874432.0000 - probe_loss: -724874432.0000 - val_loss: -418795008.0000 - val_probe_loss: -418795008.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/1000\n",
      "1000/1000 [==============================] - 1s 824us/sample - loss: -737300288.0000 - probe_loss: -737300288.0000 - val_loss: -426125120.0000 - val_probe_loss: -426125120.0000\n",
      "Epoch 223/1000\n",
      "1000/1000 [==============================] - 1s 893us/sample - loss: -749868352.0000 - probe_loss: -749868352.0000 - val_loss: -433540000.0000 - val_probe_loss: -433540000.0000\n",
      "Epoch 224/1000\n",
      "1000/1000 [==============================] - 1s 766us/sample - loss: -762578752.0000 - probe_loss: -762578752.0000 - val_loss: -441040960.0000 - val_probe_loss: -441040960.0000\n",
      "Epoch 225/1000\n",
      "1000/1000 [==============================] - 1s 830us/sample - loss: -775432448.0000 - probe_loss: -775432448.0000 - val_loss: -448628000.0000 - val_probe_loss: -448628000.0000\n",
      "Epoch 226/1000\n",
      "1000/1000 [==============================] - 1s 770us/sample - loss: -788430272.0000 - probe_loss: -788430272.0000 - val_loss: -456302080.0000 - val_probe_loss: -456302080.0000\n",
      "Epoch 227/1000\n",
      "1000/1000 [==============================] - 1s 763us/sample - loss: -801572992.0000 - probe_loss: -801572992.0000 - val_loss: -464063328.0000 - val_probe_loss: -464063328.0000\n",
      "Epoch 228/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -814861312.0000 - probe_loss: -814861312.0000 - val_loss: -471912288.0000 - val_probe_loss: -471912288.0000\n",
      "Epoch 229/1000\n",
      "1000/1000 [==============================] - 1s 850us/sample - loss: -828295936.0000 - probe_loss: -828295936.0000 - val_loss: -479849824.0000 - val_probe_loss: -479849824.0000\n",
      "Epoch 230/1000\n",
      "1000/1000 [==============================] - 1s 972us/sample - loss: -841877888.0000 - probe_loss: -841877888.0000 - val_loss: -487875360.0000 - val_probe_loss: -487875360.0000\n",
      "Epoch 231/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -855607360.0000 - probe_loss: -855607360.0000 - val_loss: -495990016.0000 - val_probe_loss: -495990016.0000\n",
      "Epoch 232/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -869485248.0000 - probe_loss: -869485248.0000 - val_loss: -504194048.0000 - val_probe_loss: -504194048.0000\n",
      "Epoch 233/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -883512512.0000 - probe_loss: -883512512.0000 - val_loss: -512488032.0000 - val_probe_loss: -512488032.0000\n",
      "Epoch 234/1000\n",
      "1000/1000 [==============================] - 1s 801us/sample - loss: -897689600.0000 - probe_loss: -897689600.0000 - val_loss: -520872416.0000 - val_probe_loss: -520872416.0000\n",
      "Epoch 235/1000\n",
      "1000/1000 [==============================] - 1s 713us/sample - loss: -912017600.0000 - probe_loss: -912017600.0000 - val_loss: -529347904.0000 - val_probe_loss: -529347904.0000\n",
      "Epoch 236/1000\n",
      "1000/1000 [==============================] - 1s 853us/sample - loss: -926497216.0000 - probe_loss: -926497216.0000 - val_loss: -537914240.0000 - val_probe_loss: -537914240.0000\n",
      "Epoch 237/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -941128832.0000 - probe_loss: -941128832.0000 - val_loss: -546572288.0000 - val_probe_loss: -546572288.0000\n",
      "Epoch 238/1000\n",
      "1000/1000 [==============================] - 1s 999us/sample - loss: -955913280.0000 - probe_loss: -955913280.0000 - val_loss: -555322432.0000 - val_probe_loss: -555322432.0000\n",
      "Epoch 239/1000\n",
      "1000/1000 [==============================] - 1s 793us/sample - loss: -970851392.0000 - probe_loss: -970851392.0000 - val_loss: -564165120.0000 - val_probe_loss: -564165120.0000\n",
      "Epoch 240/1000\n",
      "1000/1000 [==============================] - 1s 842us/sample - loss: -985943744.0000 - probe_loss: -985943744.0000 - val_loss: -573100736.0000 - val_probe_loss: -573100736.0000\n",
      "Epoch 241/1000\n",
      "1000/1000 [==============================] - 1s 737us/sample - loss: -1001191104.0000 - probe_loss: -1001191104.0000 - val_loss: -582129856.0000 - val_probe_loss: -582129856.0000\n",
      "Epoch 242/1000\n",
      "1000/1000 [==============================] - 1s 861us/sample - loss: -1016594432.0000 - probe_loss: -1016594432.0000 - val_loss: -591253120.0000 - val_probe_loss: -591253120.0000\n",
      "Epoch 243/1000\n",
      "1000/1000 [==============================] - 1s 918us/sample - loss: -1032154432.0000 - probe_loss: -1032154432.0000 - val_loss: -600470272.0000 - val_probe_loss: -600470272.0000\n",
      "Epoch 244/1000\n",
      "1000/1000 [==============================] - 1s 832us/sample - loss: -1047871488.0000 - probe_loss: -1047871488.0000 - val_loss: -609781952.0000 - val_probe_loss: -609781952.0000\n",
      "Epoch 245/1000\n",
      "1000/1000 [==============================] - 1s 765us/sample - loss: -1063746496.0000 - probe_loss: -1063746496.0000 - val_loss: -619188800.0000 - val_probe_loss: -619188800.0000\n",
      "Epoch 246/1000\n",
      "1000/1000 [==============================] - 1s 868us/sample - loss: -1079780096.0000 - probe_loss: -1079780096.0000 - val_loss: -628691392.0000 - val_probe_loss: -628691392.0000\n",
      "Epoch 247/1000\n",
      "1000/1000 [==============================] - 1s 793us/sample - loss: -1095973120.0000 - probe_loss: -1095973120.0000 - val_loss: -638289792.0000 - val_probe_loss: -638289792.0000\n",
      "Epoch 248/1000\n",
      "1000/1000 [==============================] - 1s 750us/sample - loss: -1112326144.0000 - probe_loss: -1112326144.0000 - val_loss: -647984832.0000 - val_probe_loss: -647984832.0000\n",
      "Epoch 249/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1128840704.0000 - probe_loss: -1128840704.0000 - val_loss: -657776512.0000 - val_probe_loss: -657776512.0000\n",
      "Epoch 250/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1145516288.0000 - probe_loss: -1145516288.0000 - val_loss: -667665472.0000 - val_probe_loss: -667665472.0000\n",
      "Epoch 251/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1162353664.0000 - probe_loss: -1162353664.0000 - val_loss: -677652672.0000 - val_probe_loss: -677652672.0000\n",
      "Epoch 252/1000\n",
      "1000/1000 [==============================] - 1s 886us/sample - loss: -1179354496.0000 - probe_loss: -1179354496.0000 - val_loss: -687737792.0000 - val_probe_loss: -687737792.0000\n",
      "Epoch 253/1000\n",
      "1000/1000 [==============================] - 1s 960us/sample - loss: -1196519040.0000 - probe_loss: -1196519040.0000 - val_loss: -697921024.0000 - val_probe_loss: -697921024.0000\n",
      "Epoch 254/1000\n",
      "1000/1000 [==============================] - 1s 827us/sample - loss: -1213847808.0000 - probe_loss: -1213847808.0000 - val_loss: -708203264.0000 - val_probe_loss: -708203264.0000\n",
      "Epoch 255/1000\n",
      "1000/1000 [==============================] - 1s 832us/sample - loss: -1231341696.0000 - probe_loss: -1231341696.0000 - val_loss: -718585024.0000 - val_probe_loss: -718585024.0000\n",
      "Epoch 256/1000\n",
      "1000/1000 [==============================] - 1s 761us/sample - loss: -1249001216.0000 - probe_loss: -1249001216.0000 - val_loss: -729066624.0000 - val_probe_loss: -729066624.0000\n",
      "Epoch 257/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1266827264.0000 - probe_loss: -1266827264.0000 - val_loss: -739648384.0000 - val_probe_loss: -739648384.0000\n",
      "Epoch 258/1000\n",
      "1000/1000 [==============================] - 0s 458us/sample - loss: -1284820736.0000 - probe_loss: -1284820736.0000 - val_loss: -750330752.0000 - val_probe_loss: -750330752.0000\n",
      "Epoch 259/1000\n",
      "1000/1000 [==============================] - 1s 833us/sample - loss: -1302981888.0000 - probe_loss: -1302981888.0000 - val_loss: -761114048.0000 - val_probe_loss: -761114048.0000\n",
      "Epoch 260/1000\n",
      "1000/1000 [==============================] - 1s 755us/sample - loss: -1321311616.0000 - probe_loss: -1321311616.0000 - val_loss: -771999104.0000 - val_probe_loss: -771999104.0000\n",
      "Epoch 261/1000\n",
      "1000/1000 [==============================] - 1s 835us/sample - loss: -1339810688.0000 - probe_loss: -1339810688.0000 - val_loss: -782985856.0000 - val_probe_loss: -782985856.0000\n",
      "Epoch 262/1000\n",
      "1000/1000 [==============================] - 1s 801us/sample - loss: -1358479872.0000 - probe_loss: -1358479872.0000 - val_loss: -794075072.0000 - val_probe_loss: -794075072.0000\n",
      "Epoch 263/1000\n",
      "1000/1000 [==============================] - 1s 819us/sample - loss: -1377319680.0000 - probe_loss: -1377319680.0000 - val_loss: -805266880.0000 - val_probe_loss: -805266880.0000\n",
      "Epoch 264/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 972us/sample - loss: -1396331136.0000 - probe_loss: -1396331136.0000 - val_loss: -816562816.0000 - val_probe_loss: -816562816.0000\n",
      "Epoch 265/1000\n",
      "1000/1000 [==============================] - 1s 981us/sample - loss: -1415515264.0000 - probe_loss: -1415515264.0000 - val_loss: -827961472.0000 - val_probe_loss: -827961472.0000\n",
      "Epoch 266/1000\n",
      "1000/1000 [==============================] - 1s 728us/sample - loss: -1434871552.0000 - probe_loss: -1434871552.0000 - val_loss: -839463936.0000 - val_probe_loss: -839463936.0000\n",
      "Epoch 267/1000\n",
      "1000/1000 [==============================] - 1s 668us/sample - loss: -1454401024.0000 - probe_loss: -1454401024.0000 - val_loss: -851071040.0000 - val_probe_loss: -851071040.0000\n",
      "Epoch 268/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1474105088.0000 - probe_loss: -1474105088.0000 - val_loss: -862782848.0000 - val_probe_loss: -862782848.0000\n",
      "Epoch 269/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1493983744.0000 - probe_loss: -1493983744.0000 - val_loss: -874599744.0000 - val_probe_loss: -874599744.0000\n",
      "Epoch 270/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1514038016.0000 - probe_loss: -1514038016.0000 - val_loss: -886522496.0000 - val_probe_loss: -886522496.0000\n",
      "Epoch 271/1000\n",
      "1000/1000 [==============================] - 1s 967us/sample - loss: -1534268800.0000 - probe_loss: -1534268800.0000 - val_loss: -898551424.0000 - val_probe_loss: -898551424.0000\n",
      "Epoch 272/1000\n",
      "1000/1000 [==============================] - 1s 867us/sample - loss: -1554676480.0000 - probe_loss: -1554676480.0000 - val_loss: -910686592.0000 - val_probe_loss: -910686592.0000\n",
      "Epoch 273/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1575261440.0000 - probe_loss: -1575261440.0000 - val_loss: -922928704.0000 - val_probe_loss: -922928704.0000\n",
      "Epoch 274/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -1596024960.0000 - probe_loss: -1596024960.0000 - val_loss: -935278144.0000 - val_probe_loss: -935278144.0000\n",
      "Epoch 275/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1616967296.0000 - probe_loss: -1616967296.0000 - val_loss: -947735424.0000 - val_probe_loss: -947735424.0000\n",
      "Epoch 276/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1638089344.0000 - probe_loss: -1638089344.0000 - val_loss: -960301312.0000 - val_probe_loss: -960301312.0000\n",
      "Epoch 277/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1659392512.0000 - probe_loss: -1659392512.0000 - val_loss: -972975168.0000 - val_probe_loss: -972975168.0000\n",
      "Epoch 278/1000\n",
      "1000/1000 [==============================] - 0s 384us/sample - loss: -1680875904.0000 - probe_loss: -1680875904.0000 - val_loss: -985757824.0000 - val_probe_loss: -985757824.0000\n",
      "Epoch 279/1000\n",
      "1000/1000 [==============================] - 1s 795us/sample - loss: -1702541312.0000 - probe_loss: -1702541312.0000 - val_loss: -998649792.0000 - val_probe_loss: -998649792.0000\n",
      "Epoch 280/1000\n",
      "1000/1000 [==============================] - 0s 433us/sample - loss: -1724388992.0000 - probe_loss: -1724388992.0000 - val_loss: -1011651520.0000 - val_probe_loss: -1011651520.0000\n",
      "Epoch 281/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1746419840.0000 - probe_loss: -1746419840.0000 - val_loss: -1024763456.0000 - val_probe_loss: -1024763456.0000\n",
      "Epoch 282/1000\n",
      "1000/1000 [==============================] - 1s 871us/sample - loss: -1768634368.0000 - probe_loss: -1768634368.0000 - val_loss: -1037986112.0000 - val_probe_loss: -1037986112.0000\n",
      "Epoch 283/1000\n",
      "1000/1000 [==============================] - 1s 809us/sample - loss: -1791033344.0000 - probe_loss: -1791033344.0000 - val_loss: -1051319552.0000 - val_probe_loss: -1051319552.0000\n",
      "Epoch 284/1000\n",
      "1000/1000 [==============================] - 1s 850us/sample - loss: -1813617408.0000 - probe_loss: -1813617408.0000 - val_loss: -1064764544.0000 - val_probe_loss: -1064764544.0000\n",
      "Epoch 285/1000\n",
      "1000/1000 [==============================] - 1s 775us/sample - loss: -1836387584.0000 - probe_loss: -1836387584.0000 - val_loss: -1078321152.0000 - val_probe_loss: -1078321152.0000\n",
      "Epoch 286/1000\n",
      "1000/1000 [==============================] - 1s 837us/sample - loss: -1859344512.0000 - probe_loss: -1859344512.0000 - val_loss: -1091989888.0000 - val_probe_loss: -1091989888.0000\n",
      "Epoch 287/1000\n",
      "1000/1000 [==============================] - 1s 837us/sample - loss: -1882488192.0000 - probe_loss: -1882488192.0000 - val_loss: -1105771264.0000 - val_probe_loss: -1105771264.0000\n",
      "Epoch 288/1000\n",
      "1000/1000 [==============================] - 1s 796us/sample - loss: -1905820032.0000 - probe_loss: -1905820032.0000 - val_loss: -1119666432.0000 - val_probe_loss: -1119666432.0000\n",
      "Epoch 289/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -1929340928.0000 - probe_loss: -1929340928.0000 - val_loss: -1133674112.0000 - val_probe_loss: -1133674112.0000\n",
      "Epoch 290/1000\n",
      "1000/1000 [==============================] - 1s 883us/sample - loss: -1953050496.0000 - probe_loss: -1953050496.0000 - val_loss: -1147795584.0000 - val_probe_loss: -1147795584.0000\n",
      "Epoch 291/1000\n",
      "1000/1000 [==============================] - 1s 870us/sample - loss: -1976949760.0000 - probe_loss: -1976949760.0000 - val_loss: -1162031488.0000 - val_probe_loss: -1162031488.0000\n",
      "Epoch 292/1000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: -2001040256.0000 - probe_loss: -2001040256.0000 - val_loss: -1176381824.0000 - val_probe_loss: -1176381824.0000\n",
      "Epoch 293/1000\n"
     ]
    }
   ],
   "source": [
    "#for run in range(3,9):\n",
    "#for lr in (0.001, 0.0001, 0.00001, 0.000001):\n",
    "FeedForwardTrain().run(run_n=9, lr=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view logs in tensorboard use following command in command prompt: <br>\n",
    "    tensorboard --logdir={logs_base_dir} --host=localhost <br>\n",
    "<br>\n",
    "Note: change {logs_base_dir} to the name of the folder where logs are being saved. <br>\n",
    "Note: make sure you have navigated to the directory containing this script in the command prompt before running the above command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
