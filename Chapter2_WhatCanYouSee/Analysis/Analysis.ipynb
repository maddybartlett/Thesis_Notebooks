{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "#%matplotlib notebook\n",
    "#%matplotlib qt5\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0) # bigger figures\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set() # better looking figs\n",
    "\n",
    "# hide warnings for clarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# our own set of small helper functions for plotting, etc\n",
    "from utils import plot_embedding, plot_compare_embeddings, show_heatmap, plot_confusion_matrix\n",
    "import factor_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset, rename and re-order columns where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/Input/fulldata.csv\")\n",
    "data[\"clipId\"] = data[\"clipName\"].apply(lambda x: x[-8:-6])\n",
    "\n",
    "# re-order columns + keep only useful ones\n",
    "data = data[['pptID','fileName', 'condition', 'age', 'gender', 'nationality', 'firstLang', 'trial', 'clipId', 'freetext',\n",
    " 'q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'q08', 'q09', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17',\n",
    " 'q18', 'q19', 'q20', 'q21', 'q22', 'q23', 'q24', 'q25', 'q26', 'q27', 'q28', 'q29', 'q30']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename `qXX` columns with the names of the actual constructs tested in the questionnaire.\n",
    "\n",
    "Notes:\n",
    "- `condition=1` is the 'Movement-only' condition, `condition=2` is the 'Full-scene' condition\n",
    "- each participant `pptID` watched 4 different clips, hence 4 rows per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructs=[\"Sad\", \"Happy\", \"Angry\", \"Excited\", \"Calm\", \n",
    "            \"Friendly\", \"Aggressive\", \"Engaged\", \"Distracted\", \n",
    "            \"Bored\", \"Frustrated\",\"Dominant\",\"Submissive\"]\n",
    "\n",
    "index = data.columns.tolist()\n",
    "index = index[0:10] + [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + [c for c1 in constructs for c in ['left' + c1, 'right' + c1]]\n",
    "data.columns=index\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "For each left/right pair of constructs, compute the absolute difference and the sum (shifted to [-2, +2] interval).\n",
    "\n",
    "This provides insight on the imbalance of the given construct between the children (difference), and the overall 'strength' of the construct in the clip (sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in constructs:\n",
    "    data[\"diff\"+c] = abs(data[\"left\" + c] - data[\"right\" + c]) #calculating difference\n",
    "    data[\"sum\"+c] = data[\"left\" + c] + data[\"right\" + c] - 4 #calculating sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 lists of columns names, one for diff/sum constructs, one for left/right constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsLeftRight=[]\n",
    "columnsDiffSum=[]\n",
    "\n",
    "for c in constructs:\n",
    "    columnsLeftRight.append(\"left\" + c)\n",
    "    columnsLeftRight.append(\"right\" + c)\n",
    "    \n",
    "    columnsDiffSum.append(\"diff\" + c)\n",
    "    columnsDiffSum.append(\"sum\" + c)\n",
    "\n",
    "\n",
    "# by default, work with differences & sum for each constructs\n",
    "selectedColumns=columnsDiffSum\n",
    "\n",
    "# work with differences & sum and the four questions about group dynamics\n",
    "allQuestionsDiffSum = [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + columnsDiffSum\n",
    "\n",
    "# work with left/right ratings and the four questions about group dynamics i.e. raw ratings\n",
    "allQuestionsLeftRight = [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + columnsLeftRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define several useful 'partial' views of the data.** <br>\n",
    "Dataframes for each condition. <br>\n",
    "Responses to left/right questions as dataframe and as array. <br>\n",
    "Clip names. <br>\n",
    "Mean ratings per clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL-SCENE DATA\n",
    "\n",
    "fullscene_df = data[data[\"condition\"]==2] # dataframe showing full scene data only\n",
    "\n",
    "# the responses to the 26 left/right Likert-scale questions\n",
    "fullscene_ratings_df = fullscene_df[selectedColumns].astype(float)\n",
    "fullscene = fullscene_ratings_df.values # the underlying numpy array, needed for clustering\n",
    "\n",
    "# clip names\n",
    "fullscene_labels = fullscene_df[\"clipId\"].values\n",
    "\n",
    "# mean ratings per clip\n",
    "fullscene_means = fullscene_df.groupby([\"clipId\"]).mean()[selectedColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVEMENT-ALONE DATA\n",
    "\n",
    "move_df=data[data[\"condition\"]==1] # dataframe showing movement alone data only\n",
    "\n",
    "# the responses to the 26 left/right Likert-scale questions\n",
    "move_ratings_df=move_df[selectedColumns].astype(float)\n",
    "move=move_ratings_df.values # the underlying numpy array, needed for clustering\n",
    "\n",
    "# clip names\n",
    "move_labels=move_df[\"clipId\"].values\n",
    "\n",
    "# mean ratings per clip\n",
    "move_means=move_df.groupby([\"clipId\"]).mean()[selectedColumns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographics - Table 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_age_mean = move_df['age'].mean()\n",
    "move_age_max = max(move_df['age'])\n",
    "move_age_min = min(move_df['age'])\n",
    "move_per_male = ((len(move_df['gender'][move_df['gender']=='Male'])/4)/(len(move_df)/4))*100\n",
    "move_per_female = ((len(move_df['gender'][move_df['gender']=='Female'])/4)/(len(move_df)/4))*100\n",
    "move_per_usa = ((len(move_df['nationality'][move_df['nationality']=='American'])/4)/(len(move_df)/4))*100\n",
    "move_per_eng = ((len(move_df['firstLang'][move_df['firstLang']=='English'])/4)/(len(move_df)/4))*100\n",
    "\n",
    "full_age_mean = fullscene_df['age'].mean()\n",
    "full_age_max = max(fullscene_df['age'])\n",
    "full_age_min = min(fullscene_df['age'])\n",
    "full_per_male = ((len(fullscene_df['gender'][fullscene_df['gender']=='Male'])/4)/(len(fullscene_df)/4))*100\n",
    "full_per_female = ((len(fullscene_df['gender'][fullscene_df['gender']=='Female'])/4)/(len(fullscene_df)/4))*100\n",
    "full_per_usa = ((len(fullscene_df['nationality'][fullscene_df['nationality']=='American'])/4)/(len(fullscene_df)/4))*100\n",
    "full_per_eng = ((len(fullscene_df['firstLang'][fullscene_df['firstLang']=='English'])/4)/(len(fullscene_df)/4))*100\n",
    "\n",
    "all_age_mean = data['age'].mean()\n",
    "all_age_max = max(data['age'])\n",
    "all_age_min = min(data['age'])\n",
    "all_per_male = ((len(data['gender'][data['gender']=='Male'])/4)/(len(data)/4))*100\n",
    "all_per_female = ((len(data['gender'][data['gender']=='Female'])/4)/(len(data)/4))*100\n",
    "all_per_usa = ((len(data['nationality'][data['nationality']=='American'])/4)/(len(data)/4))*100\n",
    "all_per_eng = ((len(data['firstLang'][data['firstLang']=='English'])/4)/(len(data)/4))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Percent_Correct_Frames = pd.DataFrame({'N': [len(move_df)/4, len(fullscene_df)/4, (len(data)/4)],\n",
    "                                      'Mean Age (Range)': [f'{move_age_mean:.2f}'+' ('+f'{move_age_min}'+'-'+f'{move_age_max}'+')', \n",
    "                                                           f'{full_age_mean:.2f}'+' ('+f'{full_age_min}'+'-'+f'{full_age_max}'+')', \n",
    "                                                           f'{all_age_mean:.2f}'+' ('+f'{all_age_min}'+'-'+f'{all_age_max}'+')'],\n",
    "                                      'Gender %M %F': [f'{move_per_male:.2f}'+'%'+', '+f'{move_per_female:.2f}'+'%',\n",
    "                                                      f'{full_per_male:.2f}'+'%'+', '+f'{full_per_female:.2f}'+'%',\n",
    "                                                      f'{all_per_male:.2f}'+'%'+', '+f'{all_per_female:.2f}'+'%'],\n",
    "                                      '% American': [f'{move_per_usa:.0f}'+'%',\n",
    "                                                    f'{full_per_usa:.0f}'+'%',\n",
    "                                                    f'{all_per_usa:.0f}'+'%'],\n",
    "                                      '% English First Language': [f'{move_per_eng:.0f}'+'%',\n",
    "                                                    f'{full_per_eng:.0f}'+'%',\n",
    "                                                    f'{all_per_eng:.0f}'+'%']})\n",
    "\n",
    "Percent_Correct_Frames.set_index([pd.Index(['Movement-Alone', 'Full-Scene', 'Both'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.1 Inter-Rater Agreement\n",
    "\n",
    "Calculate Kirppendorff's alpha to look at how much participants in each condition agreed on their ratings for each clip (lighter color means higher agreement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "\n",
    "krip={}\n",
    "\n",
    "# agreement for fullscene condition for all questions\n",
    "for clipName, group in fullscene_df[[\"clipId\"] + allQuestionsLeftRight].groupby([\"clipId\"]): # working with all raw ratings\n",
    "    krip[clipName]=(krippendorff.alpha(group.values[:,1:].astype(int),level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "# agreement for movement-alone condition for all questions\n",
    "for clipName, group in move_df[[\"clipId\"] + allQuestionsLeftRight].groupby(\"clipId\"):\n",
    "    krip[clipName]=krip[clipName] + (krippendorff.alpha(group.values[:,1:].astype(int),level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "# create random ratings and calculate \"baseline\" agreement score\n",
    "for clipName, group in move_df[[\"clipId\"] + allQuestionsLeftRight].groupby(\"clipId\"):\n",
    "    ratings = group.values[:,1:].astype(int)\n",
    "    ratings = np.random.randint(0,5,ratings.shape)\n",
    "    krip[clipName]=krip[clipName] + (krippendorff.alpha(ratings,level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "    \n",
    "krippendorff_df=pd.DataFrame.from_dict(krip,orient=\"index\", columns=[\"Fullscene alpha\", \"N\", \"Movement-Alone alpha\", \"N\",\"Random ratings alpha\", \"N\"])\n",
    "\n",
    "show_heatmap(krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\", \"Random ratings alpha\"]].round(3), cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: did participants in the fullscene condition agree more in their ratings of each clip than participants in the movement-alone condition?\n",
    "\n",
    "We test this by comparing the agreement scores in the fullscene vs movement-alone videos, using an paired samples T-test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from math import sqrt\n",
    "\n",
    "fullscene_krip = krippendorff_df[\"Fullscene alpha\"]\n",
    "move_krip = krippendorff_df[\"Movement-Alone alpha\"]\n",
    "\n",
    "print('Mean Kripp Alpha Fullscene:', fullscene_krip.mean())\n",
    "print('SD:', fullscene_krip.std())\n",
    "print('Mean Kripp Alpha Movement:', move_krip.mean())\n",
    "print('SD:', move_krip.std())\n",
    "\n",
    "print('Paired Samples T-Test:', ttest_rel(fullscene_krip, move_krip))\n",
    "\n",
    "cohens_d = (fullscene_krip.mean() - move_krip.mean()) / (sqrt((fullscene_krip.std() ** 2 + move_krip.std() ** 2) / 2))\n",
    "\n",
    "print(\"Cohen's d:\", cohens_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-test comparing krippendorf's alpha in the movement alone condition against 0 to see if there is agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "print('One Sample T-Test:', ttest_1samp(move_krip, 0))\n",
    "\n",
    "cohens_d = (move_krip.mean()-0) / move_krip.std()\n",
    "\n",
    "print(\"Cohen's d:\", cohens_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.2 Automatic Labelling of Internal States\n",
    "\n",
    "Multi-label classification using k-Nearest Neighbours (k=3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "training_ground_truth = { '01': ['Aggressive'],\n",
    "                         '02': ['Excited', 'Aggressive', 'Aimless'],\n",
    "                         '03': ['Excited', 'Fun'],\n",
    "                         '04': ['Cooperative'],\n",
    "                         '05': ['Bored', 'Aimless'],\n",
    "                         '06': ['Cooperative'],\n",
    "                         '07': ['Dominant'],\n",
    "                         '08': ['Bored', 'Fun'],\n",
    "                         '09': ['Cooperative'],\n",
    "                         '10': ['Cooperative', 'Dominant'],\n",
    "                         '11': ['Cooperative', 'Dominant'],\n",
    "                         '12': ['Aggressive', 'Aimless'],\n",
    "                         '13': ['Excited', 'Aggressive', 'Aimless'],\n",
    "                         '14': ['Aggressive'],\n",
    "                         '15': ['Dominant'],\n",
    "                         '16': ['Cooperative', 'Dominant'],\n",
    "                         '17': ['Excited', 'Aggressive'],\n",
    "                         '18': ['Aggressive', 'Dominant'],\n",
    "                         '19': ['Dominant'],\n",
    "                         '20': ['Excited']}\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(training_ground_truth.values())\n",
    "\n",
    "def create_datasets(training=data, testing=None, cols=allQuestionsDiffSum, test_size=0.2, use_clip_id_as_label=False, random_labels=False, random_state=None):\n",
    "    \"\"\"Returns a training dataset and training labels, and a testing dataset and testing labels.\n",
    "    \n",
    "    If testing is None, it randomly splits the training dataframe (at test_size).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if testing is None:\n",
    "        \n",
    "        if use_clip_id_as_label:\n",
    "            labels = list(training[\"clipId\"].map(int))\n",
    "        else:\n",
    "            labels = []\n",
    "            for id in training[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[id])\n",
    "\n",
    "        data = training[cols].values\n",
    "\n",
    "        training_data, testing_data, training_labels, testing_labels = train_test_split(data, labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        if not use_clip_id_as_label:\n",
    "            \n",
    "            training_labels, testing_labels = mlb.transform(training_labels), mlb.transform(testing_labels)\n",
    "            \n",
    "            if random_labels:\n",
    "                for labels in training_labels:\n",
    "                    np.random.shuffle(labels)                 \n",
    "                np.random.shuffle(training_labels)             \n",
    "            \n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if use_clip_id_as_label:\n",
    "            training_labels = list(training[\"clipId\"].map(int))\n",
    "            testing_labels = list(testing[\"clipId\"].map(int))\n",
    "        else:\n",
    "            labels = []\n",
    "            for id in training[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[id])\n",
    "\n",
    "            training_labels = mlb.transform(labels)\n",
    "\n",
    "            labels = []\n",
    "            for id in testing[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[id])\n",
    "\n",
    "            testing_labels = mlb.transform(labels)\n",
    "\n",
    "            if random_labels:\n",
    "                if random_labels:\n",
    "                    for labels in training_labels:\n",
    "                        np.random.shuffle(labels)                 \n",
    "                    np.random.shuffle(training_labels) \n",
    "\n",
    "        \n",
    "        training_data = training[cols].values\n",
    "        testing_data = testing[cols].values\n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def train(training_data, training_labels):\n",
    "    \n",
    "    #clf = RandomForestClassifier()\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    #clf = ExtraTreeClassifier(random_state=0)\n",
    "\n",
    "\n",
    "    clf.fit(training_data, training_labels)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def predict(clf, testing_data, inverse_transform_labels=True):\n",
    "    p = clf.predict(testing_data)\n",
    "    if inverse_transform_labels:\n",
    "        return mlb.inverse_transform(p) \n",
    "    else:\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics as metrics\n",
    "    \n",
    "def run_classification(training, \n",
    "                       testing=None, \n",
    "                       cols=allQuestionsDiffSum, \n",
    "                       use_clip_id_as_label=False, \n",
    "                       random_labels=False,\n",
    "                       crossvalidation_iterations=50):\n",
    "    \"\"\"\n",
    "    Metrics for multi-label classification coming form Sorower, Mohammad S. \"A literature survey on algorithms for multi-label learning.\" Oregon State University, Corvallis (2010).\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\"Accuracy\": [],\n",
    "               \"Precision\": [],\n",
    "               \"Recall\": [],\n",
    "               \"F1-measure\": []}              \n",
    "    labels_f1 = []\n",
    "    \n",
    "\n",
    "    for x in range(crossvalidation_iterations):\n",
    "               \n",
    "        training_data, testing_data, training_labels, testing_labels = create_datasets(training=training, \n",
    "                                                                                       testing=testing, \n",
    "                                                                                       cols=cols, \n",
    "                                                                                       use_clip_id_as_label=use_clip_id_as_label,\n",
    "                                                                                       random_labels=random_labels,\n",
    "                                                                                       random_state = x)\n",
    "        \n",
    "        if x == 0:\n",
    "            print(\"Shape of training data: %s\" % str(training_data.shape))\n",
    "            print(\"Shape of testing data: %s\" % str(testing_data.shape))\n",
    "        \n",
    "        clf = train(training_data, training_labels)\n",
    "\n",
    "        pred_labels = predict(clf, testing_data, inverse_transform_labels = not use_clip_id_as_label)\n",
    "\n",
    "        \n",
    "        at_least_one = 0\n",
    "        at_least_one_no_incorrect = 0\n",
    "        \n",
    "        if not use_clip_id_as_label:\n",
    "            \n",
    "            nb_classes = len(mlb.classes_)\n",
    "            \n",
    "            \n",
    "            labels_f1.append(dict(zip(mlb.classes_, metrics.f1_score(testing_labels, mlb.transform(pred_labels), average=None))))\n",
    "            \n",
    "            results[\"Accuracy\"].append(metrics.accuracy_score(testing_labels, mlb.transform(pred_labels)))\n",
    "            results[\"Recall\"].append(metrics.recall_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            results[\"Precision\"].append(metrics.precision_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            results[\"F1-measure\"].append(metrics.f1_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            \n",
    "            \n",
    "            \n",
    "            testing_labels = mlb.inverse_transform(testing_labels)\n",
    "            \n",
    "            exact = 0\n",
    "            accuracy = 0\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "            f1_measure = 0\n",
    "            \n",
    "            for actual, pred in zip(testing_labels, pred_labels):\n",
    "                \n",
    "                pred = set(pred)\n",
    "                actual = set(actual)\n",
    "                \n",
    "                if len(pred) == 0: continue\n",
    "                    \n",
    "                if pred == actual:\n",
    "                    #print(\"%s <-> %s\" % (actual, pred))\n",
    "                    exact += 1\n",
    "                    \n",
    "                intersection = pred.intersection(actual)\n",
    "                union = pred.union(actual)\n",
    "\n",
    "                #accuracy += float(len(intersection)) / len(union)\n",
    "                #precision += float(len(intersection)) / len(pred)\n",
    "                #recall += float(len(intersection)) / len(actual)\n",
    "                #f1_measure += 2 * float(len(intersection)) / (len(pred) + len(actual))\n",
    "                \n",
    "            \n",
    "            #results[\"exact\"].append(float(exact) / len(testing_labels))\n",
    "            #results[\"accuracy\"].append(accuracy / len(testing_labels))\n",
    "            #results[\"precision\"].append(precision / len(testing_labels))\n",
    "            #results[\"recall\"].append(recall / len(testing_labels))\n",
    "            #results[\"f1_measure\"].append(f1_measure / len(testing_labels))\n",
    "            \n",
    "            \n",
    "            \n",
    "        else: # use_clip_id_as_label = True\n",
    "            # does not make much sense as at_least_one & at_least_one_no_incorrect are the same as 'exact'\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(results), pd.DataFrame(labels_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the significance of the classification results, by computing a permutation-based p-value.\n",
    "\n",
    "*This method is based on Ojala and Garriga 2010 \"Permutation Tests for Studying Classifier Performance\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_value_permutation(dataset, testing=None, k=10, crossvalidation_iterations=50):\n",
    "    \n",
    "    pvalues = []\n",
    "    \n",
    "    for x in range(crossvalidation_iterations):\n",
    "        times_baseline_worst = 0\n",
    "\n",
    "        training_data, testing_data, training_labels, testing_labels = create_datasets(training=dataset,\n",
    "                                                                                       testing=testing,\n",
    "                                                                                       random_state=x)\n",
    "\n",
    "        # train the classifier\n",
    "        clf = train(training_data, training_labels)\n",
    "\n",
    "        # baseline prediction\n",
    "        pred_labels = predict(clf, testing_data, inverse_transform_labels=False)\n",
    "\n",
    "        baseline_error = 1 - metrics.f1_score(testing_labels, pred_labels, average='weighted')\n",
    "\n",
    "        for i in range(k):\n",
    "            testing_labels = np.random.permutation(testing_labels)\n",
    "\n",
    "            error = 1 - metrics.f1_score(testing_labels, pred_labels, average='weighted')\n",
    "\n",
    "            if error <= baseline_error:\n",
    "                times_baseline_worst += 1\n",
    "\n",
    "        pvalues.append((times_baseline_worst + 1)/float(k + 1))\n",
    "    return pd.Series(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene classification - 80%/20% split - multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_fs = run_classification(fullscene_df, crossvalidation_iterations=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at accuracy, precision, recall and f1 scores for overall performance on every label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also computed F1 score for each classification label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_f1_fs.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_p_value_permutation(fullscene_df, k=100, crossvalidation_iterations=300).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene classification - 80%/20% split - multi-label - CHANCE level\n",
    "\n",
    "The chance level is computed by associating random labels to the testing samples (still following the same distribution of labels as found in the original dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_chance, labels_f1_fschance = run_classification(fullscene_df, random_labels=True, crossvalidation_iterations=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_chance.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_f1_fschance.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene training; movement alone testing - multi-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_ma = run_classification(fullscene_df, testing=move_df, crossvalidation_iterations=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_f1_ma.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_p_value_permutation(fullscene_df, testing=move_df, k=100, crossvalidation_iterations=300).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene training; movement alone testing - multi-labels - CHANCE level\n",
    "\n",
    "The chance level is computed by associating random labels to the testing samples (still following the same distribution of labels as found in the original dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_machance = run_classification(fullscene_df, testing=move_df, random_labels=True, crossvalidation_iterations=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_f1_machance.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure of mean F1-score for each label in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = labels_f1_fs.describe()\n",
    "ma = labels_f1_ma.describe()\n",
    "\n",
    "fschance = labels_f1_fschance.describe()\n",
    "machance = labels_f1_machance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mean = fs.iloc[[1]].T\n",
    "ma_mean = ma.iloc[[1]].T\n",
    "\n",
    "f1_mean = pd.concat([fs_mean['mean'], ma_mean['mean']], axis=1, keys=['fullscene', 'movement alone'])\n",
    "f1_mean\n",
    "ax = f1_mean.plot.bar(rot=0, figsize=(10,5), color=['black', 'grey']) #plot\n",
    "ax.set_ylabel(\"Mean F1 Score\", fontsize=16)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "ax.figure.savefig('../Figs/f1labelscore.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mean_chance = fschance.iloc[[1]].T\n",
    "ma_mean_chance = machance.iloc[[1]].T\n",
    "f1_mean_chance = pd.concat([fs_mean['mean'], fs_mean_chance['mean'], ma_mean['mean'], ma_mean_chance['mean']], axis=1, keys=['Fullscene', 'Fullscene Chance', 'Movement Alone', 'Movement Alone Chance'])\n",
    "\n",
    "(f1_mean_chance).round(3).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.3 Factor Analysis\n",
    "\n",
    "Exploratory Factor Analysis examining what latent constructs underlie particiants' responses in each condition.\n",
    "\n",
    "The Python factor_analyzer module is a port of EFA from the R' psych package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable, total = factor_analyzer.factor_analyzer.calculate_kmo(fullscene_ratings_df)\n",
    "print('KMO statistic Fullscene: ', total)\n",
    "      \n",
    "variable, total = factor_analyzer.factor_analyzer.calculate_kmo(move_ratings_df)\n",
    "print('KMO statistic Movement: ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bartlett Test Fullscene: ', factor_analyzer.factor_analyzer.calculate_bartlett_sphericity(fullscene_ratings_df))\n",
    "print('Bartlett Test Movement: ', factor_analyzer.factor_analyzer.calculate_bartlett_sphericity(move_ratings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = 'promax'\n",
    "\n",
    "nb_factors=3\n",
    "\n",
    "efa_fullscene = factor_analyzer.FactorAnalyzer()\n",
    "efa_fullscene.analyze(fullscene_ratings_df, nb_factors, rotation=rotation)\n",
    "fullscene_loadings=efa_fullscene.loadings\n",
    "\n",
    "efa_move = factor_analyzer.FactorAnalyzer()\n",
    "efa_move.analyze(move_ratings_df, nb_factors, rotation=rotation)\n",
    "move_loadings=efa_move.loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_fullscene.get_factor_variance() #variance explained by each construct for fullscene data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_move.get_factor_variance() #variance explained by each construct for movement-alone data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the loadings for the *fullscene* vs the *movement alone* data show that the first three factors are highly correlated. **This shows that, using factor analysis, we have uncovered latent constructs that are used by participants to describe the clips in both *fullscene* and *movement alone* conditions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge loadings into one dataframe, movement alone and fullscene side-by-side\n",
    "loadings=pd.concat([fullscene_loadings, move_loadings], keys=[\"fullscene\",\"movement alone\"], axis=1)\n",
    "loadings=loadings.swaplevel(0,1,1).sort_index(1)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"Pearson correlation between factors 'fullscene' vs 'movement alone'\")\n",
    "for i in range(1, nb_factors+1):\n",
    "    r, p=pearsonr(loadings[\"Factor%d\" % i][\"fullscene\"].values, loadings[\"Factor%d\" % i][\"movement alone\"].values)\n",
    "    print(\"Factor %d: r=%f, p=%f\" % (i,r,p)) \n",
    "    \n",
    "\n",
    "show_heatmap(loadings[abs(loadings)>=0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Expressivness of the EFA Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFA embeddings\n",
    "\n",
    "We can use the EFA space as a 'better' space to represent our clips, where the latent, composite constructs correspond to the main axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "\n",
    "nb_of_factors=3\n",
    "fullscene_efa = np.dot(fullscene,fullscene_loadings.values[:,:nb_of_factors])\n",
    "fullscene_means_efa = np.dot(fullscene_means,fullscene_loadings.values[:,:nb_of_factors])\n",
    "move_efa = np.dot(move,fullscene_loadings.values[:,:nb_of_factors])\n",
    "move_means_efa = np.dot(move_means,fullscene_loadings.values[:,:nb_of_factors])\n",
    "\n",
    "move_pure_efa = np.dot(move,move_loadings.values[:,:nb_of_factors])\n",
    "move_pure_means_efa = np.dot(move_means,move_loadings.values[:,:nb_of_factors])\n",
    "\n",
    "\n",
    "plot_embedding(fullscene_efa, fullscene_labels,fullscene_means_efa, fullscene_means.index, title=\"EFA-space embedding of the fullscene data\", three_d=True)\n",
    "plot_embedding(move_efa, move_labels,move_means_efa, move_means.index, title=\"EFA-space embedding of the movement alone data (EFA on fullscene data)\", three_d=False)\n",
    "plot_embedding(move_pure_efa, move_labels,move_pure_means_efa, move_means.index, title=\"EFA-space embedding of the movement alone data (EFA on movement alone data)\", three_d=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, even if the EFA factors are quite similar, the distances between the same clips in fullscene vs movement alone data are high in the EFA space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_efa=pd.DataFrame(np.power(np.sum(np.power(move_means_efa - fullscene_means_efa, 2), axis=1), 0.5), index=move_means.index, columns=[\"distance_efa\"])\n",
    "\n",
    "print(\"Mean distance:\\n%s\" % distances_efa.mean(axis=0))\n",
    "show_heatmap(distances_efa, cmap=\"summer_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the EFA projections to the original dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullscene_df[\"efa1\"] = pd.Series(fullscene_efa[:,0], index=fullscene_df.index)\n",
    "fullscene_df[\"efa2\"] = pd.Series(fullscene_efa[:,1], index=fullscene_df.index)\n",
    "fullscene_df[\"efa3\"] = pd.Series(fullscene_efa[:,2], index=fullscene_df.index)\n",
    "move_df[\"efa1\"] = pd.Series(move_efa[:,0], index=move_df.index)\n",
    "move_df[\"efa2\"] = pd.Series(move_efa[:,1], index=move_df.index)\n",
    "move_df[\"efa3\"] = pd.Series(move_efa[:,2], index=move_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then re-classify the clips, comparing the performance of the original 26-dimensional ratings to the 3-dimensional EFA-space projections (*still using a 300-fold cross-validation)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 300\n",
    "\n",
    "print(\"Fullscene, 80%/20%...\")\n",
    "results_fullscene,labels_f1_fullscene = run_classification(fullscene_df, crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene, 80%/20%, EFA space...\")\n",
    "results_fullscene_efa,labels_f1_fullscene_efa = run_classification(fullscene_df, cols=[\"efa1\", \"efa2\", \"efa3\"], crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene, chance level...\")\n",
    "results_fullscene_chance,labels_f1_fullscene_chance = run_classification(fullscene_df, random_labels=True, crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene, 80%/20%, sanity check [input cols=['age']]...\")\n",
    "results_fullscene_age,labels_f1_fullscene_age = run_classification(fullscene_df, cols=[\"age\"], crossvalidation_iterations=nb_iterations)\n",
    "\n",
    "print(\"Fullscene vs skeletons...\")\n",
    "results_fullscene_move,labels_f1_move = run_classification(fullscene_df, testing=move_df, crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene vs skeletons, EFA space...\")\n",
    "results_fullscene_move_efa,labels_f1_move_efa = run_classification(fullscene_df, testing=move_df, cols=[\"efa1\", \"efa2\", \"efa3\"], crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene vs skeletons, chance level...\")\n",
    "results_fullscene_move_chance,labels_f1_move_chance = run_classification(fullscene_df, testing=move_df, random_labels=True, crossvalidation_iterations=nb_iterations)\n",
    "\n",
    "collated_results = pd.DataFrame({\"Full-scene, EFA space\": results_fullscene_efa.mean(),\n",
    "                                 \"Full-scene\": results_fullscene.mean(),\n",
    "                                 \"Full-scene, chance\": results_fullscene_chance.mean(),\n",
    "                                 #\"Full-scene-80-20-sanity-check\": results_fullscene_age.mean(),\n",
    "                                 \"Movement-alone, EFA space\": results_fullscene_move_efa.mean(),\n",
    "                                 \"Movement-alone\": results_fullscene_move.mean(),\n",
    "                                 \"Movement-alone, chance\": results_fullscene_move_chance.mean()})\n",
    "labels_f1 = pd.concat({\"Full-scene, EFA space\": labels_f1_fullscene_efa,\n",
    "                       \"Full-scene\": labels_f1_fullscene,\n",
    "                       \"Full-scene, chance\": labels_f1_fullscene_chance,\n",
    "                       #\"fullscene-80-20-sanity-check\": labels_f1_fullscene_age,\n",
    "                       \"Movement-alone, EFA\": labels_f1_move_efa,\n",
    "                       \"Movement-alone\": labels_f1_move,\n",
    "                       \"Movement-alone, chance\": labels_f1_move_chance}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated_results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullscene = labels_f1.loc[:, 'Full-scene']\n",
    "fullsceneEFA = labels_f1.loc[:, 'Full-scene, EFA space']\n",
    "fullsceneChance = labels_f1.loc[:, 'Full-scene, chance']\n",
    "movement = labels_f1.loc[:, 'Movement-alone']\n",
    "movementEFA = labels_f1.loc[:, 'Movement-alone, EFA']\n",
    "movementChance = labels_f1.loc[:, 'Movement-alone, chance']\n",
    "\n",
    "collated_labels = pd.DataFrame([fullsceneEFA.mean(axis=0)])\n",
    "collated_labels = collated_labels.append([fullscene.mean(axis=0)],ignore_index=True)\n",
    "collated_labels = collated_labels.append([fullsceneChance.mean(axis=0)],ignore_index=True)\n",
    "collated_labels = collated_labels.append([movementEFA.mean(axis=0)],ignore_index=True)\n",
    "collated_labels = collated_labels.append([movement.mean(axis=0)],ignore_index=True)\n",
    "collated_labels = collated_labels.append([movementChance.mean(axis=0)],ignore_index=True)\n",
    "\n",
    "collated_labels.rename(index={0:'Fullscene, EFA',1:'Fullscene',2:'Chance',\n",
    "                              3:'Movement alone, EFA',4:'Movement alone',5:'Chance'}, inplace=True)\n",
    "collated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
