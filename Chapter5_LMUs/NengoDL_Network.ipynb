{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime, os\n",
    "from itertools import islice\n",
    "from IPython.display import clear_output\n",
    "import pytry\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import nengo_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Nengo Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow version 2.2.0 <br>\n",
    "Nengo dl version 3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTrain(pytry.Trial):\n",
    "    def params(self):\n",
    "        self.param('run_n', run_n=1)\n",
    "        self.param('theta', theta=3)\n",
    "        self.param('q', q=4)\n",
    "    \n",
    "    def evaluate(self, param):       \n",
    "        #LOAD/SET PARAMETERS\n",
    "        run_n=param.run_n     \n",
    "        theta=param.theta  \n",
    "        q=param.q  \n",
    "        learn_rate=0.000001 #learning rate\n",
    "        n_epoch=1000 #number of training epochs\n",
    "        train_rat=0.7 #ratio of training to testing data\n",
    "        \n",
    "        print(run_n) #print run number\n",
    "        np.random.seed() #set seed to random value\n",
    "        \n",
    "#################################################################\n",
    "        #Set and create data directories\n",
    "        data_dir=\"D:\\\\NEN002\\\\GridSearch_NDL_LMU\\\\\"+str(q)+'_'+str(theta)+\"\\\\\" #directory to save data to\n",
    "        \n",
    "        if run_n == 0:\n",
    "            os.mkdir(data_dir)\n",
    "            os.mkdir(\"D:\\\\NEN002\\\\GridSearch_NDL_LMU_logs\\\\\"+str(q)+'_'+str(theta)+\"\\\\\")\n",
    "            \n",
    "        logdir = os.path.join(\"D:\\\\NEN002\\\\GridSearch_NDL_LMU_logs\\\\\"+str(q)+'_'+str(theta))+\"\\\\\"+str(run_n) #directory to save logs to for tensorboard\n",
    "        \n",
    "        #load data\n",
    "        highengdata = np.load('high_lmu'+str(q)+'_theta'+str(theta)+'.npy',allow_pickle=True) #np.load('higheng.npy', allow_pickle=True)\n",
    "        lowengdata = np.load('low_lmu'+str(q)+'_theta'+str(theta)+'.npy',allow_pickle=True) #np.load('loweng.npy', allow_pickle=True)\n",
    "##################################################################\n",
    "\n",
    "        high=list(highengdata[:])\n",
    "        low=list(lowengdata[:])\n",
    "\n",
    "        #shuffle clips\n",
    "        np.random.shuffle(high)\n",
    "        np.random.shuffle(low)\n",
    "\n",
    "        #get training clips and convert to list of frames\n",
    "        high_train = high[:(int(len(low)*train_rat))]\n",
    "        high_train=list(np.vstack(high_train))\n",
    "        low_train = low[:(int(len(low)*train_rat))]\n",
    "        low_train=list(np.vstack(low_train))\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(high_train)\n",
    "        np.random.shuffle(low_train)\n",
    "\n",
    "        #Make each training set same size (low is shortest)\n",
    "        high_train = high_train[:(int(len(low_train)))]\n",
    "        low_train = low_train[:(int(len(low_train)))]\n",
    "\n",
    "        #Create 10% validation dataset\n",
    "        high_val = high[(int(len(low)*0.9)):(int(len(low)*1))]\n",
    "        low_val = low[(int(len(low)*0.9)):(int(len(low)*1))]\n",
    "        \n",
    "        #convert to list of frames\n",
    "        high_val=list(np.vstack(high_val))\n",
    "        low_val=list(np.vstack(low_val))\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(high_val)\n",
    "        np.random.shuffle(low_val)\n",
    "        \n",
    "        #create test sets\n",
    "        #get 30% of clips for testing\n",
    "        high_test = high[(int(len(low)*train_rat)):(int(len(low)*0.9))]\n",
    "        low_test = low[(int(len(low)*train_rat)):(int(len(low)*0.9))]\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(high_test)\n",
    "        np.random.shuffle(low_test)\n",
    "\n",
    "        #GENERATE INPUT MATRICES\n",
    "        #Concatenate high and low sets together to create a single array for training, test and validation separately\n",
    "        all_train = np.vstack(np.concatenate((high_train, low_train)))\n",
    "        all_test = np.concatenate((high_test, low_test)) \n",
    "        all_val = np.concatenate((high_val, low_val))\n",
    "\n",
    "        #create the target data for training\n",
    "        target_train = np.zeros((all_train.shape[0],2))\n",
    "        n_high = len(high_train)\n",
    "        target_train[:n_high,0] = 1 #target for high = [1,0]\n",
    "        target_train[n_high:,1] = 1 #target for low = [0,1]\n",
    "        \n",
    "        #and for validation\n",
    "        target_val = np.zeros((all_val.shape[0],2))\n",
    "        n_high = len(high_val)\n",
    "        target_val[:n_high,0] = 1\n",
    "        target_val[n_high:,1] = 1\n",
    "        \n",
    "        #SAVE TRAINING AND TESTING DATA\n",
    "        pickle_filename = (data_dir+\"/%s_training_data.pkl\") % str(run_n) #(data_dir+\"/%s_training_data\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(all_train, file)\n",
    "\n",
    "        pickle_filename = (data_dir+\"/%s_testing_data.pkl\") % str(run_n) #(data_dir+\"/%s_testing_data\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(all_test, file)\n",
    "\n",
    "        #BUILD MODEL (input layer -> hidden layer -> output layer)\n",
    "        N = 200\n",
    "        seed = 1\n",
    "        N_dims = len(all_train[0]) \n",
    "\n",
    "        model = nengo.Network(seed=seed)\n",
    "        with model:    \n",
    "            input = nengo.Node(np.zeros(N_dims))\n",
    "            hidden1 = nengo.Ensemble(n_neurons=N, dimensions=N_dims, radius=np.sqrt(N_dims), \n",
    "                                     neuron_type=nengo.RectifiedLinear())\n",
    "            nengo.Connection(input, hidden1, synapse=None)\n",
    "            output = nengo.Node(None, size_in=2)\n",
    "            nengo.Connection(hidden1, output, eval_points=all_train, function=target_train, \n",
    "                             scale_eval_points=False, synapse=None)\n",
    "\n",
    "            p_output = nengo.Probe(output)\n",
    "        \n",
    "        #TRAINING DATA AND PARAMETERS\n",
    "        minibatch_size = 1000       # this can be adjusted to speed up training\n",
    "                                    # Note: with larger minibatch_size, you may need to reduce this\n",
    "                                    #(if training is making things worse, then you need to reduce this!)\n",
    "                                    # number of iterations through the training data to perform\n",
    "\n",
    "        batches = int(np.ceil(len(all_train)/minibatch_size)) #number of training batches needed (length of data / minibatch size)\n",
    "        val_batches = int(np.ceil(len(all_val)/minibatch_size)) #number of validation batches needed\n",
    "\n",
    "        training_data_input = np.array(all_train, copy=True)\n",
    "        order = np.arange(len(training_data_input))\n",
    "        np.random.shuffle(order)\n",
    "        training_data_input_rand = training_data_input[order]\n",
    "        training_data_input_rand.resize(minibatch_size,batches,N_dims)\n",
    "        training_data_input.resize(minibatch_size,batches,N_dims)\n",
    "\n",
    "        training_data_output = np.array(target_train, copy=True)\n",
    "        training_data_output_rand = training_data_output[order]\n",
    "        training_data_output.resize(minibatch_size,batches,2)\n",
    "        training_data_output_rand.resize(minibatch_size,batches,2)\n",
    "        \n",
    "        val_input = np.array(all_val, copy=True)\n",
    "        order = np.arange(len(val_input))\n",
    "        np.random.shuffle(order)\n",
    "        val_input_rand = val_input[order]\n",
    "        val_input_rand.resize(minibatch_size,val_batches,N_dims)\n",
    "        val_input.resize(minibatch_size,val_batches,N_dims)\n",
    "        \n",
    "        val_output = np.array(target_val, copy=True)\n",
    "        val_output_rand = val_output[order]\n",
    "        val_output.resize(minibatch_size,val_batches,2)\n",
    "        val_output_rand.resize(minibatch_size,val_batches,2)\n",
    "        \n",
    "        #set learning objective (defines loss)\n",
    "        def objective(outputs, targets): \n",
    "            return tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=outputs, labels=targets)  \n",
    "        \n",
    "        #TRAINING\n",
    "        with nengo_dl.Simulator(\n",
    "                model, minibatch_size=minibatch_size) as sim:\n",
    "            sim.compile(optimizer=tf.optimizers.Adam(learn_rate),\n",
    "                loss={p_output: objective})\n",
    "            sim.fit(training_data_input_rand, training_data_output_rand, epochs=n_epoch,\n",
    "                   callbacks=[tf.keras.callbacks.TensorBoard(log_dir=logdir)],\n",
    "                   validation_data=(val_input_rand, val_output_rand))\n",
    "            sim.save_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n #('./trained.data') \n",
    "            \n",
    "        #Run model with training data, using trained weights\n",
    "        input.output = nengo.processes.PresentInput(all_train, presentation_time=0.001)\n",
    "        with nengo_dl.Simulator(model, minibatch_size=1) as sim:\n",
    "            sim.load_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n\n",
    "            sim.run(len(all_train)*0.001)\n",
    "            \n",
    "        #Save data\n",
    "        pickle_filename = (data_dir+\"/%s_train_target.pkl\") % str(run_n) #(data_dir+\"/%s_train_target\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(target_train, file)\n",
    "        \n",
    "        pickle_filename = (data_dir+\"/%s_train_trange.pkl\") % str(run_n) #(data_dir+\"/%s_train_trange\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.trange(), file)\n",
    "            \n",
    "        pickle_filename = (data_dir+\"/%s_train_out.pkl\") % str(run_n) #(data_dir+\"/%s_train_out\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.data[p_output][0], file)    \n",
    "        \n",
    "        test_out=[]\n",
    "        target_test=[]\n",
    "        for j in range(len(all_test)):\n",
    "            if j<int(len(high_test)):\n",
    "                target = np.zeros((all_test[j].shape[0],2))\n",
    "                target[:,0] = 1\n",
    "            else:\n",
    "                target = np.zeros((all_test[j].shape[0],2))\n",
    "                target[:,1] = 1\n",
    "            input.output = nengo.processes.PresentInput(all_test[j], presentation_time=0.001)\n",
    "            with nengo_dl.Simulator(model, minibatch_size=1) as sim:\n",
    "                sim.load_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n\n",
    "                sim.run(len(all_test[j])*0.001)\n",
    "            test_out.append(sim.data[p_output][0])\n",
    "            target_test.append(target)\n",
    "            \n",
    "        #Save data\n",
    "        pickle_filename = (data_dir+\"/%s_test_target.pkl\") % str(run_n) #(data_dir+\"/%s_test_target\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(target_test, file)\n",
    "        \n",
    "        pickle_filename = (data_dir+\"/%s_test_trange.pkl\") % str(run_n) #(data_dir+\"/%s_test_trange\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.trange(), file)\n",
    "            \n",
    "        pickle_filename = (data_dir+\"/%s_test_out.pkl\") % str(run_n) #(data_dir+\"/%s_test_out\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(test_out, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running FeedForwardTrain#20200421-150059-632be95d\n",
      "7\n",
      "|#####################Building network (57%)                     | ETA: 0:00:00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbartlett2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nengo_dl\\simulator.py:460: UserWarning: No GPU support detected. It is recommended that you install tensorflow-gpu (`pip install tensorflow-gpu`).\n",
      "  \"No GPU support detected. It is recommended that you \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:01                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7872 - probe_loss: 0.7872 - val_loss: 0.7919 - val_probe_loss: 0.7919\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.7710 - probe_loss: 0.7710 - val_loss: 0.7757 - val_probe_loss: 0.7757\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7547 - probe_loss: 0.7547 - val_loss: 0.7594 - val_probe_loss: 0.7594\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7385 - probe_loss: 0.7385 - val_loss: 0.7432 - val_probe_loss: 0.7432\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7222 - probe_loss: 0.7222 - val_loss: 0.7270 - val_probe_loss: 0.7270\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7060 - probe_loss: 0.7060 - val_loss: 0.7108 - val_probe_loss: 0.7108\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6897 - probe_loss: 0.6897 - val_loss: 0.6945 - val_probe_loss: 0.6945\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.6735 - probe_loss: 0.6735 - val_loss: 0.6783 - val_probe_loss: 0.6783\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.6572 - probe_loss: 0.6572 - val_loss: 0.6621 - val_probe_loss: 0.6621\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6410 - probe_loss: 0.6410 - val_loss: 0.6458 - val_probe_loss: 0.6458\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.6247 - probe_loss: 0.6247 - val_loss: 0.6296 - val_probe_loss: 0.6296\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.6085 - probe_loss: 0.6085 - val_loss: 0.6134 - val_probe_loss: 0.6134\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5922 - probe_loss: 0.5922 - val_loss: 0.5972 - val_probe_loss: 0.5972\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.5760 - probe_loss: 0.5760 - val_loss: 0.5809 - val_probe_loss: 0.5809\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.5597 - probe_loss: 0.5597 - val_loss: 0.5647 - val_probe_loss: 0.5647\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5435 - probe_loss: 0.5435 - val_loss: 0.5485 - val_probe_loss: 0.5485\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.5272 - probe_loss: 0.5272 - val_loss: 0.5323 - val_probe_loss: 0.5323\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.5110 - probe_loss: 0.5110 - val_loss: 0.5160 - val_probe_loss: 0.5160\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4948 - probe_loss: 0.4948 - val_loss: 0.4998 - val_probe_loss: 0.4998\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.4785 - probe_loss: 0.4785 - val_loss: 0.4836 - val_probe_loss: 0.4836\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4623 - probe_loss: 0.4623 - val_loss: 0.4674 - val_probe_loss: 0.4674\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.4460 - probe_loss: 0.4460 - val_loss: 0.4511 - val_probe_loss: 0.4511\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.4298 - probe_loss: 0.4298 - val_loss: 0.4349 - val_probe_loss: 0.4349\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4135 - probe_loss: 0.4135 - val_loss: 0.4187 - val_probe_loss: 0.4187\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3973 - probe_loss: 0.3973 - val_loss: 0.4024 - val_probe_loss: 0.4024\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3810 - probe_loss: 0.3810 - val_loss: 0.3862 - val_probe_loss: 0.3862\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3648 - probe_loss: 0.3648 - val_loss: 0.3700 - val_probe_loss: 0.3700\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3485 - probe_loss: 0.3485 - val_loss: 0.3538 - val_probe_loss: 0.3538\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3323 - probe_loss: 0.3323 - val_loss: 0.3375 - val_probe_loss: 0.3375\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.3160 - probe_loss: 0.3160 - val_loss: 0.3213 - val_probe_loss: 0.3213\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2998 - probe_loss: 0.2998 - val_loss: 0.3051 - val_probe_loss: 0.3051\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2835 - probe_loss: 0.2835 - val_loss: 0.2889 - val_probe_loss: 0.2889\n",
      "Epoch 33/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.2673 - probe_loss: 0.2673 - val_loss: 0.2726 - val_probe_loss: 0.2726\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.2510 - probe_loss: 0.2510 - val_loss: 0.2564 - val_probe_loss: 0.2564\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2348 - probe_loss: 0.2348 - val_loss: 0.2402 - val_probe_loss: 0.2402\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2185 - probe_loss: 0.2185 - val_loss: 0.2240 - val_probe_loss: 0.2240\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2023 - probe_loss: 0.2023 - val_loss: 0.2077 - val_probe_loss: 0.2077\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1860 - probe_loss: 0.1860 - val_loss: 0.1915 - val_probe_loss: 0.1915\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1698 - probe_loss: 0.1698 - val_loss: 0.1753 - val_probe_loss: 0.1753\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1535 - probe_loss: 0.1535 - val_loss: 0.1591 - val_probe_loss: 0.1591\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1373 - probe_loss: 0.1373 - val_loss: 0.1428 - val_probe_loss: 0.1428\n",
      "Epoch 42/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1211 - probe_loss: 0.1211 - val_loss: 0.1266 - val_probe_loss: 0.1266\n",
      "Epoch 43/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1048 - probe_loss: 0.1048 - val_loss: 0.1104 - val_probe_loss: 0.1104\n",
      "Epoch 44/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.0886 - probe_loss: 0.0886 - val_loss: 0.0941 - val_probe_loss: 0.0941\n",
      "Epoch 45/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.0723 - probe_loss: 0.0723 - val_loss: 0.0779 - val_probe_loss: 0.0779\n",
      "Epoch 46/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.0561 - probe_loss: 0.0561 - val_loss: 0.0617 - val_probe_loss: 0.0617\n",
      "Epoch 47/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.0398 - probe_loss: 0.0398 - val_loss: 0.0455 - val_probe_loss: 0.0455\n",
      "Epoch 48/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.0236 - probe_loss: 0.0236 - val_loss: 0.0292 - val_probe_loss: 0.0292\n",
      "Epoch 49/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.0073 - probe_loss: 0.0073 - val_loss: 0.0130 - val_probe_loss: 0.0130\n",
      "Epoch 50/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.0089 - probe_loss: -0.0089 - val_loss: -0.0032 - val_probe_loss: -0.0032\n",
      "Epoch 51/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -0.0252 - probe_loss: -0.0252 - val_loss: -0.0194 - val_probe_loss: -0.0194\n",
      "Epoch 52/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.0414 - probe_loss: -0.0414 - val_loss: -0.0357 - val_probe_loss: -0.0357\n",
      "Epoch 53/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.0577 - probe_loss: -0.0577 - val_loss: -0.0519 - val_probe_loss: -0.0519\n",
      "Epoch 54/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -0.0739 - probe_loss: -0.0739 - val_loss: -0.0681 - val_probe_loss: -0.0681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.0902 - probe_loss: -0.0902 - val_loss: -0.0843 - val_probe_loss: -0.0843\n",
      "Epoch 56/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.1064 - probe_loss: -0.1064 - val_loss: -0.1006 - val_probe_loss: -0.1006\n",
      "Epoch 57/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.1227 - probe_loss: -0.1227 - val_loss: -0.1168 - val_probe_loss: -0.1168\n",
      "Epoch 58/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.1389 - probe_loss: -0.1389 - val_loss: -0.1330 - val_probe_loss: -0.1330\n",
      "Epoch 59/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.1552 - probe_loss: -0.1552 - val_loss: -0.1493 - val_probe_loss: -0.1493\n",
      "Epoch 60/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.1714 - probe_loss: -0.1714 - val_loss: -0.1655 - val_probe_loss: -0.1655\n",
      "Epoch 61/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.1877 - probe_loss: -0.1877 - val_loss: -0.1817 - val_probe_loss: -0.1817\n",
      "Epoch 62/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.2039 - probe_loss: -0.2039 - val_loss: -0.1979 - val_probe_loss: -0.1979\n",
      "Epoch 63/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -0.2202 - probe_loss: -0.2202 - val_loss: -0.2142 - val_probe_loss: -0.2142\n",
      "Epoch 64/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.2364 - probe_loss: -0.2364 - val_loss: -0.2304 - val_probe_loss: -0.2304\n",
      "Epoch 65/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -0.2527 - probe_loss: -0.2527 - val_loss: -0.2466 - val_probe_loss: -0.2466\n",
      "Epoch 66/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.2689 - probe_loss: -0.2689 - val_loss: -0.2628 - val_probe_loss: -0.2628\n",
      "Epoch 67/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.2851 - probe_loss: -0.2851 - val_loss: -0.2791 - val_probe_loss: -0.2791\n",
      "Epoch 68/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.3014 - probe_loss: -0.3014 - val_loss: -0.2953 - val_probe_loss: -0.2953\n",
      "Epoch 69/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.3176 - probe_loss: -0.3176 - val_loss: -0.3115 - val_probe_loss: -0.3115\n",
      "Epoch 70/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.3339 - probe_loss: -0.3339 - val_loss: -0.3277 - val_probe_loss: -0.3277\n",
      "Epoch 71/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.3501 - probe_loss: -0.3501 - val_loss: -0.3440 - val_probe_loss: -0.3440\n",
      "Epoch 72/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.3664 - probe_loss: -0.3664 - val_loss: -0.3602 - val_probe_loss: -0.3602\n",
      "Epoch 73/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.3826 - probe_loss: -0.3826 - val_loss: -0.3764 - val_probe_loss: -0.3764\n",
      "Epoch 74/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.3989 - probe_loss: -0.3989 - val_loss: -0.3927 - val_probe_loss: -0.3927\n",
      "Epoch 75/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.4151 - probe_loss: -0.4151 - val_loss: -0.4089 - val_probe_loss: -0.4089\n",
      "Epoch 76/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.4314 - probe_loss: -0.4314 - val_loss: -0.4251 - val_probe_loss: -0.4251\n",
      "Epoch 77/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.4476 - probe_loss: -0.4476 - val_loss: -0.4413 - val_probe_loss: -0.4413\n",
      "Epoch 78/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.4639 - probe_loss: -0.4639 - val_loss: -0.4576 - val_probe_loss: -0.4576\n",
      "Epoch 79/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.4801 - probe_loss: -0.4801 - val_loss: -0.4738 - val_probe_loss: -0.4738\n",
      "Epoch 80/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.4964 - probe_loss: -0.4964 - val_loss: -0.4900 - val_probe_loss: -0.4900\n",
      "Epoch 81/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.5126 - probe_loss: -0.5126 - val_loss: -0.5062 - val_probe_loss: -0.5062\n",
      "Epoch 82/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.5289 - probe_loss: -0.5289 - val_loss: -0.5225 - val_probe_loss: -0.5225\n",
      "Epoch 83/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.5451 - probe_loss: -0.5451 - val_loss: -0.5387 - val_probe_loss: -0.5387\n",
      "Epoch 84/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.5614 - probe_loss: -0.5614 - val_loss: -0.5549 - val_probe_loss: -0.5549\n",
      "Epoch 85/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.5776 - probe_loss: -0.5776 - val_loss: -0.5711 - val_probe_loss: -0.5711\n",
      "Epoch 86/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -0.5939 - probe_loss: -0.5939 - val_loss: -0.5874 - val_probe_loss: -0.5874\n",
      "Epoch 87/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.6101 - probe_loss: -0.6101 - val_loss: -0.6036 - val_probe_loss: -0.6036\n",
      "Epoch 88/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.6264 - probe_loss: -0.6264 - val_loss: -0.6198 - val_probe_loss: -0.6198\n",
      "Epoch 89/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.6426 - probe_loss: -0.6426 - val_loss: -0.6361 - val_probe_loss: -0.6361\n",
      "Epoch 90/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.6588 - probe_loss: -0.6588 - val_loss: -0.6523 - val_probe_loss: -0.6523\n",
      "Epoch 91/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.6751 - probe_loss: -0.6751 - val_loss: -0.6685 - val_probe_loss: -0.6685\n",
      "Epoch 92/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.6913 - probe_loss: -0.6913 - val_loss: -0.6847 - val_probe_loss: -0.6847\n",
      "Epoch 93/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.7076 - probe_loss: -0.7076 - val_loss: -0.7010 - val_probe_loss: -0.7010\n",
      "Epoch 94/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.7238 - probe_loss: -0.7238 - val_loss: -0.7172 - val_probe_loss: -0.7172\n",
      "Epoch 95/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.7401 - probe_loss: -0.7401 - val_loss: -0.7334 - val_probe_loss: -0.7334\n",
      "Epoch 96/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.7563 - probe_loss: -0.7563 - val_loss: -0.7496 - val_probe_loss: -0.7496\n",
      "Epoch 97/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.7726 - probe_loss: -0.7726 - val_loss: -0.7659 - val_probe_loss: -0.7659\n",
      "Epoch 98/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -0.7888 - probe_loss: -0.7888 - val_loss: -0.7821 - val_probe_loss: -0.7821\n",
      "Epoch 99/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.8051 - probe_loss: -0.8051 - val_loss: -0.7983 - val_probe_loss: -0.7983\n",
      "Epoch 100/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.8213 - probe_loss: -0.8213 - val_loss: -0.8145 - val_probe_loss: -0.8145\n",
      "Epoch 101/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.8376 - probe_loss: -0.8376 - val_loss: -0.8308 - val_probe_loss: -0.8308\n",
      "Epoch 102/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.8538 - probe_loss: -0.8538 - val_loss: -0.8470 - val_probe_loss: -0.8470\n",
      "Epoch 103/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.8701 - probe_loss: -0.8701 - val_loss: -0.8632 - val_probe_loss: -0.8632\n",
      "Epoch 104/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.8863 - probe_loss: -0.8863 - val_loss: -0.8795 - val_probe_loss: -0.8795\n",
      "Epoch 105/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.9026 - probe_loss: -0.9026 - val_loss: -0.8957 - val_probe_loss: -0.8957\n",
      "Epoch 106/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.9188 - probe_loss: -0.9188 - val_loss: -0.9119 - val_probe_loss: -0.9119\n",
      "Epoch 107/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.9351 - probe_loss: -0.9351 - val_loss: -0.9281 - val_probe_loss: -0.9281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.9513 - probe_loss: -0.9513 - val_loss: -0.9444 - val_probe_loss: -0.9444\n",
      "Epoch 109/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.9676 - probe_loss: -0.9676 - val_loss: -0.9606 - val_probe_loss: -0.9606\n",
      "Epoch 110/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -0.9838 - probe_loss: -0.9838 - val_loss: -0.9768 - val_probe_loss: -0.9768\n",
      "Epoch 111/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -1.0001 - probe_loss: -1.0001 - val_loss: -0.9930 - val_probe_loss: -0.9930\n",
      "Epoch 112/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.0163 - probe_loss: -1.0163 - val_loss: -1.0093 - val_probe_loss: -1.0093\n",
      "Epoch 113/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.0326 - probe_loss: -1.0326 - val_loss: -1.0255 - val_probe_loss: -1.0255\n",
      "Epoch 114/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -1.0488 - probe_loss: -1.0488 - val_loss: -1.0417 - val_probe_loss: -1.0417\n",
      "Epoch 115/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.0650 - probe_loss: -1.0650 - val_loss: -1.0579 - val_probe_loss: -1.0579\n",
      "Epoch 116/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.0813 - probe_loss: -1.0813 - val_loss: -1.0742 - val_probe_loss: -1.0742\n",
      "Epoch 117/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.0975 - probe_loss: -1.0975 - val_loss: -1.0904 - val_probe_loss: -1.0904\n",
      "Epoch 118/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -1.1138 - probe_loss: -1.1138 - val_loss: -1.1066 - val_probe_loss: -1.1066\n",
      "Epoch 119/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.1300 - probe_loss: -1.1300 - val_loss: -1.1229 - val_probe_loss: -1.1229\n",
      "Epoch 120/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.1463 - probe_loss: -1.1463 - val_loss: -1.1391 - val_probe_loss: -1.1391\n",
      "Epoch 121/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.1625 - probe_loss: -1.1625 - val_loss: -1.1553 - val_probe_loss: -1.1553\n",
      "Epoch 122/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.1788 - probe_loss: -1.1788 - val_loss: -1.1715 - val_probe_loss: -1.1715\n",
      "Epoch 123/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.1950 - probe_loss: -1.1950 - val_loss: -1.1878 - val_probe_loss: -1.1878\n",
      "Epoch 124/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.2113 - probe_loss: -1.2113 - val_loss: -1.2040 - val_probe_loss: -1.2040\n",
      "Epoch 125/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.2275 - probe_loss: -1.2275 - val_loss: -1.2202 - val_probe_loss: -1.2202\n",
      "Epoch 126/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.2438 - probe_loss: -1.2438 - val_loss: -1.2364 - val_probe_loss: -1.2364\n",
      "Epoch 127/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -1.2600 - probe_loss: -1.2600 - val_loss: -1.2527 - val_probe_loss: -1.2527\n",
      "Epoch 128/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.2763 - probe_loss: -1.2763 - val_loss: -1.2689 - val_probe_loss: -1.2689\n",
      "Epoch 129/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.2925 - probe_loss: -1.2925 - val_loss: -1.2851 - val_probe_loss: -1.2851\n",
      "Epoch 130/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.3088 - probe_loss: -1.3088 - val_loss: -1.3013 - val_probe_loss: -1.3013\n",
      "Epoch 131/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.3250 - probe_loss: -1.3250 - val_loss: -1.3176 - val_probe_loss: -1.3176\n",
      "Epoch 132/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.3413 - probe_loss: -1.3413 - val_loss: -1.3338 - val_probe_loss: -1.3338\n",
      "Epoch 133/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.3575 - probe_loss: -1.3575 - val_loss: -1.3500 - val_probe_loss: -1.3500\n",
      "Epoch 134/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.3738 - probe_loss: -1.3738 - val_loss: -1.3663 - val_probe_loss: -1.3663\n",
      "Epoch 135/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.3900 - probe_loss: -1.3900 - val_loss: -1.3825 - val_probe_loss: -1.3825\n",
      "Epoch 136/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.4063 - probe_loss: -1.4063 - val_loss: -1.3987 - val_probe_loss: -1.3987\n",
      "Epoch 137/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.4225 - probe_loss: -1.4225 - val_loss: -1.4149 - val_probe_loss: -1.4149\n",
      "Epoch 138/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.4387 - probe_loss: -1.4387 - val_loss: -1.4312 - val_probe_loss: -1.4312\n",
      "Epoch 139/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.4550 - probe_loss: -1.4550 - val_loss: -1.4474 - val_probe_loss: -1.4474\n",
      "Epoch 140/1000\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: -1.4712 - probe_loss: -1.4712 - val_loss: -1.4636 - val_probe_loss: -1.4636\n",
      "Epoch 141/1000\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: -1.4875 - probe_loss: -1.4875 - val_loss: -1.4798 - val_probe_loss: -1.4798\n",
      "Epoch 142/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.5037 - probe_loss: -1.5037 - val_loss: -1.4961 - val_probe_loss: -1.4961\n",
      "Epoch 143/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.5200 - probe_loss: -1.5200 - val_loss: -1.5123 - val_probe_loss: -1.5123\n",
      "Epoch 144/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.5362 - probe_loss: -1.5362 - val_loss: -1.5285 - val_probe_loss: -1.5285\n",
      "Epoch 145/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.5525 - probe_loss: -1.5525 - val_loss: -1.5447 - val_probe_loss: -1.5447\n",
      "Epoch 146/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.5687 - probe_loss: -1.5687 - val_loss: -1.5610 - val_probe_loss: -1.5610\n",
      "Epoch 147/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.5850 - probe_loss: -1.5850 - val_loss: -1.5772 - val_probe_loss: -1.5772\n",
      "Epoch 148/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.6012 - probe_loss: -1.6012 - val_loss: -1.5934 - val_probe_loss: -1.5934\n",
      "Epoch 149/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.6175 - probe_loss: -1.6175 - val_loss: -1.6097 - val_probe_loss: -1.6097\n",
      "Epoch 150/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.6337 - probe_loss: -1.6337 - val_loss: -1.6259 - val_probe_loss: -1.6259\n",
      "Epoch 151/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.6500 - probe_loss: -1.6500 - val_loss: -1.6421 - val_probe_loss: -1.6421\n",
      "Epoch 152/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.6662 - probe_loss: -1.6662 - val_loss: -1.6583 - val_probe_loss: -1.6583\n",
      "Epoch 153/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.6825 - probe_loss: -1.6825 - val_loss: -1.6746 - val_probe_loss: -1.6746\n",
      "Epoch 154/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.6987 - probe_loss: -1.6987 - val_loss: -1.6908 - val_probe_loss: -1.6908\n",
      "Epoch 155/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.7150 - probe_loss: -1.7150 - val_loss: -1.7070 - val_probe_loss: -1.7070\n",
      "Epoch 156/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.7312 - probe_loss: -1.7312 - val_loss: -1.7232 - val_probe_loss: -1.7232\n",
      "Epoch 157/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.7475 - probe_loss: -1.7475 - val_loss: -1.7395 - val_probe_loss: -1.7395\n",
      "Epoch 158/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.7637 - probe_loss: -1.7637 - val_loss: -1.7557 - val_probe_loss: -1.7557\n",
      "Epoch 159/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.7800 - probe_loss: -1.7800 - val_loss: -1.7719 - val_probe_loss: -1.7719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.7962 - probe_loss: -1.7962 - val_loss: -1.7881 - val_probe_loss: -1.7881\n",
      "Epoch 161/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.8125 - probe_loss: -1.8125 - val_loss: -1.8044 - val_probe_loss: -1.8044\n",
      "Epoch 162/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.8287 - probe_loss: -1.8287 - val_loss: -1.8206 - val_probe_loss: -1.8206\n",
      "Epoch 163/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -1.8449 - probe_loss: -1.8449 - val_loss: -1.8368 - val_probe_loss: -1.8368\n",
      "Epoch 164/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.8612 - probe_loss: -1.8612 - val_loss: -1.8531 - val_probe_loss: -1.8531\n",
      "Epoch 165/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.8774 - probe_loss: -1.8774 - val_loss: -1.8693 - val_probe_loss: -1.8693\n",
      "Epoch 166/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.8937 - probe_loss: -1.8937 - val_loss: -1.8855 - val_probe_loss: -1.8855\n",
      "Epoch 167/1000\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: -1.9099 - probe_loss: -1.9099 - val_loss: -1.9017 - val_probe_loss: -1.9017\n",
      "Epoch 168/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.9262 - probe_loss: -1.9262 - val_loss: -1.9180 - val_probe_loss: -1.9180\n",
      "Epoch 169/1000\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: -1.9424 - probe_loss: -1.9424 - val_loss: -1.9342 - val_probe_loss: -1.9342\n",
      "Epoch 170/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.9587 - probe_loss: -1.9587 - val_loss: -1.9504 - val_probe_loss: -1.9504\n",
      "Epoch 171/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.9749 - probe_loss: -1.9749 - val_loss: -1.9666 - val_probe_loss: -1.9666\n",
      "Epoch 172/1000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: -1.9912 - probe_loss: -1.9912 - val_loss: -1.9829 - val_probe_loss: -1.9829\n",
      "Epoch 173/1000\n"
     ]
    }
   ],
   "source": [
    "FeedForwardTrain().run(run_n=7, q=2, theta=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for run in range(5):\n",
    "for theta in (1,3,5,7): #1,3,5,7\n",
    "    FeedForwardTrain().run(run_n=5, q=2, theta=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view logs in tensorboard use following command in command prompt: <br>\n",
    "    tensorboard --logdir={logs_base_dir} --host=localhost <br>\n",
    "<br>\n",
    "Note: change {logs_base_dir} to the name of the folder where logs are being saved. <br>\n",
    "Note: make sure you have navigated to the directory containing this script in the command prompt before running the above command."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
