{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime, os\n",
    "from itertools import islice\n",
    "from IPython.display import clear_output\n",
    "import pytry\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import nengo_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Nengo Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow version 2.2.0 <br>\n",
    "Nengo dl version 3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTrain(pytry.Trial):\n",
    "    def params(self):\n",
    "        self.param('run_n', run_n=1)\n",
    "    \n",
    "    def evaluate(self, param):       \n",
    "        #LOAD/SET PARAMETERS\n",
    "        run_n=param.run_n      \n",
    "        learn_rate=0.000001 #learning rate\n",
    "        n_epoch=200 #number of training epochs\n",
    "        train_rat=0.7 #ratio of training to testing data\n",
    "        \n",
    "        print(run_n) #print run number\n",
    "        np.random.seed() #set seed to random value\n",
    "        \n",
    "#################################################################\n",
    "        #Set and create data directories\n",
    "        data_dir=\"./NengoDL_LMU/\" #directory to save data to\n",
    "        \n",
    "        if run_n == 0:\n",
    "            os.mkdir(data_dir)\n",
    "            os.mkdir('.\\\\NengoDL_LMU_logs\\\\')\n",
    "            \n",
    "        logdir = os.path.join(\".\\\\NengoDL_LMU_logs\\\\\"+str(run_n)) #directory to save logs to for tensorboard\n",
    "        \n",
    "        #load data\n",
    "        highengdata = np.load('high_lmu4_theta3.npy',allow_pickle=True) #np.load('higheng.npy', allow_pickle=True)\n",
    "        lowengdata = np.load('low_lmu4_theta3.npy',allow_pickle=True) #np.load('loweng.npy', allow_pickle=True)\n",
    "##################################################################\n",
    "\n",
    "        high=list(highengdata[:])\n",
    "        low=list(lowengdata[:])\n",
    "\n",
    "        #shuffle clips\n",
    "        np.random.shuffle(high)\n",
    "        np.random.shuffle(low)\n",
    "\n",
    "        #get training clips and convert to list of frames\n",
    "        high_train = high[:(int(len(low)*train_rat))]\n",
    "        high_train=list(np.vstack(high_train))\n",
    "        low_train = low[:(int(len(low)*train_rat))]\n",
    "        low_train=list(np.vstack(low_train))\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(high_train)\n",
    "        np.random.shuffle(low_train)\n",
    "\n",
    "        #Make each training set same size (noplay is shortest)\n",
    "        high_train = high_train[:(int(len(low_train)))]\n",
    "        low_train = low_train[:(int(len(low_train)))]\n",
    "\n",
    "        #Create 10% validation dataset\n",
    "        goal_val = goal[(int(len(noplay)*0.9)):(int(len(noplay)*1))]\n",
    "        noplay_val = noplay[(int(len(noplay)*0.9)):(int(len(noplay)*1))]\n",
    "        \n",
    "        #convert to list of frames\n",
    "        goal_val=list(np.vstack(goal_val))\n",
    "        noplay_val=list(np.vstack(noplay_val))\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(goal_val)\n",
    "        np.random.shuffle(noplay_val)\n",
    "        \n",
    "        #create test sets\n",
    "        #get 30% of clips for testing\n",
    "        goal_test = goal[(int(len(noplay)*train_rat)):(int(len(noplay)*0.9))]\n",
    "        noplay_test = noplay[(int(len(noplay)*train_rat)):(int(len(noplay)*0.9))]\n",
    "\n",
    "        #shuffle frames\n",
    "        np.random.shuffle(goal_test)\n",
    "        np.random.shuffle(noplay_test)\n",
    "\n",
    "        #GENERATE INPUT MATRICES\n",
    "        #Concatenate goal and noplay sets together to create a single array for training, test and validation separately\n",
    "        all_train = np.vstack(np.concatenate((high_train, low_train)))\n",
    "        all_test = np.concatenate((goal_test, noplay_test)) \n",
    "        all_val = np.concatenate((goal_val, noplay_val))\n",
    "\n",
    "        #create the target data for training\n",
    "        target_train = np.zeros((all_train.shape[0],2))\n",
    "        n_goal = len(goal_train)\n",
    "        target_train[:n_goal,0] = 1 #target for goal = [1,0]\n",
    "        target_train[n_goal:,1] = 1 #target for noplay = [0,1]\n",
    "        \n",
    "        #and for validation\n",
    "        target_val = np.zeros((all_val.shape[0],2))\n",
    "        n_goal = len(goal_val)\n",
    "        target_val[:n_goal,0] = 1\n",
    "        target_val[n_goal:,1] = 1\n",
    "        \n",
    "        #SAVE TRAINING AND TESTING DATA\n",
    "        pickle_filename = (data_dir+\"/%s_training_data.pkl\") % str(run_n) #(data_dir+\"/%s_training_data\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(all_train, file)\n",
    "\n",
    "        pickle_filename = (data_dir+\"/%s_testing_data.pkl\") % str(run_n) #(data_dir+\"/%s_testing_data\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(all_test, file)\n",
    "\n",
    "        #BUILD MODEL (input layer -> hidden layer -> output layer)\n",
    "        N = 200\n",
    "        seed = 1\n",
    "        N_dims = len(all_train[0]) \n",
    "\n",
    "        model = nengo.Network(seed=seed)\n",
    "        with model:    \n",
    "            input = nengo.Node(np.zeros(N_dims))\n",
    "            hidden1 = nengo.Ensemble(n_neurons=N, dimensions=N_dims, radius=np.sqrt(N_dims), \n",
    "                                     neuron_type=nengo.RectifiedLinear())\n",
    "            nengo.Connection(input, hidden1, synapse=None)\n",
    "            output = nengo.Node(None, size_in=2)\n",
    "            nengo.Connection(hidden1, output, eval_points=all_train, function=target_train, \n",
    "                             scale_eval_points=False, synapse=None)\n",
    "\n",
    "            p_output = nengo.Probe(output)\n",
    "        \n",
    "        #TRAINING DATA AND PARAMETERS\n",
    "        minibatch_size = 1000       # this can be adjusted to speed up training\n",
    "                                    # Note: with larger minibatch_size, you may need to reduce this\n",
    "                                    #(if training is making things worse, then you need to reduce this!)\n",
    "                                    # number of iterations through the training data to perform\n",
    "\n",
    "        batches = int(np.ceil(len(all_train)/minibatch_size)) #number of training batches needed (length of data / minibatch size)\n",
    "        val_batches = int(np.ceil(len(all_val)/minibatch_size)) #number of validation batches needed\n",
    "\n",
    "        training_data_input = np.array(all_train, copy=True)\n",
    "        order = np.arange(len(training_data_input))\n",
    "        np.random.shuffle(order)\n",
    "        training_data_input_rand = training_data_input[order]\n",
    "        training_data_input_rand.resize(minibatch_size,batches,N_dims)\n",
    "        training_data_input.resize(minibatch_size,batches,N_dims)\n",
    "\n",
    "        training_data_output = np.array(target_train, copy=True)\n",
    "        training_data_output_rand = training_data_output[order]\n",
    "        training_data_output.resize(minibatch_size,batches,2)\n",
    "        training_data_output_rand.resize(minibatch_size,batches,2)\n",
    "        \n",
    "        val_input = np.array(all_val, copy=True)\n",
    "        order = np.arange(len(val_input))\n",
    "        np.random.shuffle(order)\n",
    "        val_input_rand = val_input[order]\n",
    "        val_input_rand.resize(minibatch_size,val_batches,N_dims)\n",
    "        val_input.resize(minibatch_size,val_batches,N_dims)\n",
    "        \n",
    "        val_output = np.array(target_val, copy=True)\n",
    "        val_output_rand = val_output[order]\n",
    "        val_output.resize(minibatch_size,val_batches,2)\n",
    "        val_output_rand.resize(minibatch_size,val_batches,2)\n",
    "        \n",
    "        #set learning objective (defines loss)\n",
    "        def objective(outputs, targets): \n",
    "            return tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=outputs, labels=targets)  \n",
    "        \n",
    "        #TRAINING\n",
    "        with nengo_dl.Simulator(\n",
    "                model, minibatch_size=minibatch_size) as sim:\n",
    "            sim.compile(optimizer=tf.optimizers.Adam(learn_rate),\n",
    "                loss={p_output: objective})\n",
    "            sim.fit(training_data_input_rand, training_data_output_rand, epochs=n_epoch,\n",
    "                   callbacks=[tf.keras.callbacks.TensorBoard(log_dir=logdir)],\n",
    "                   validation_data=(val_input_rand, val_output_rand))\n",
    "            sim.save_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n #('./trained.data') \n",
    "            \n",
    "        #Run model with training data, using trained weights\n",
    "        input.output = nengo.processes.PresentInput(all_train, presentation_time=0.001)\n",
    "        with nengo_dl.Simulator(model, minibatch_size=1) as sim:\n",
    "            sim.load_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n\n",
    "            sim.run(len(all_train)*0.001)\n",
    "            \n",
    "        #Save data\n",
    "        pickle_filename = (data_dir+\"/%s_train_target.pkl\") % str(run_n) #(data_dir+\"/%s_train_target\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(target_train, file)\n",
    "        \n",
    "        pickle_filename = (data_dir+\"/%s_train_trange.pkl\") % str(run_n) #(data_dir+\"/%s_train_trange\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.trange(), file)\n",
    "            \n",
    "        pickle_filename = (data_dir+\"/%s_train_out.pkl\") % str(run_n) #(data_dir+\"/%s_train_out\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.data[p_output][0], file)    \n",
    "        \n",
    "        test_out=[]\n",
    "        target_test=[]\n",
    "        for j in range(len(all_test)):\n",
    "            if j<int(len(goal_test)):\n",
    "                target = np.zeros((all_test[j].shape[0],2))\n",
    "                target[:,0] = 1\n",
    "            else:\n",
    "                target = np.zeros((all_test[j].shape[0],2))\n",
    "                target[:,1] = 1\n",
    "            input.output = nengo.processes.PresentInput(all_test[j], presentation_time=0.001)\n",
    "            with nengo_dl.Simulator(model, minibatch_size=1) as sim:\n",
    "                sim.load_params(data_dir+\"/%s_trained.data\" % str(run_n)) #(data_dir+\"/%s\"+str(test_param)+\"_trained.data\") % run_n\n",
    "                sim.run(len(all_test[j])*0.001)\n",
    "            test_out.append(sim.data[p_output][0])\n",
    "            target_test.append(target)\n",
    "            \n",
    "        #Save data\n",
    "        pickle_filename = (data_dir+\"/%s_test_target.pkl\") % str(run_n) #(data_dir+\"/%s_test_target\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(target_test, file)\n",
    "        \n",
    "        pickle_filename = (data_dir+\"/%s_test_trange.pkl\") % str(run_n) #(data_dir+\"/%s_test_trange\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(sim.trange(), file)\n",
    "            \n",
    "        pickle_filename = (data_dir+\"/%s_test_out.pkl\") % str(run_n) #(data_dir+\"/%s_test_out\"+str(test_param)+\".pkl\") % run_n\n",
    "        with open(pickle_filename, 'wb') as file:\n",
    "            pickle.dump(test_out, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(20):\n",
    "    FeedForwardTrain().run(run_n=run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view logs in tensorboard use following command in command prompt: <br>\n",
    "    tensorboard --logdir={logs_base_dir} --host=localhost <br>\n",
    "<br>\n",
    "Note: change {logs_base_dir} to the name of the folder where logs are being saved. <br>\n",
    "Note: make sure you have navigated to the directory containing this script in the command prompt before running the above command."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
